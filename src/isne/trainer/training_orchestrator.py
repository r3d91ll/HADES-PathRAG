"""
ISNE Training Orchestrator.

This module provides the orchestration logic for training the ISNE model using document
embeddings and relationships generated by the document processing pipeline. It handles
the complete training workflow from data loading to model evaluation and persistence.
"""

import os
import json
import time
import logging
import argparse
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Any, Optional, Union, Tuple, Set

import torch
import torch.nn as nn
import torch.optim as optim
from torch import Tensor
import numpy as np
from torch_geometric.data import Data, Dataset
from torch_geometric.utils import add_self_loops, to_undirected
from tqdm import tqdm

# Import ISNE components
from src.isne.models.isne_model import ISNEModel
from src.isne.training.trainer import ISNETrainer

# Import centralized type definitions
from src.types.isne import (
    ISNEConfig,
    ISNEModelConfig,
    ISNETrainingConfig,
    ISNEGraphConfig,
    ISNEDirectoriesConfig,
    DocumentType,
    RelationType,
    IngestDocument,
    DocumentRelation
)

# Import configuration utilities
try:
    from src.config.config_loader import load_pipeline_config
    CONFIG_LOADER_AVAILABLE = True
except ImportError:
    CONFIG_LOADER_AVAILABLE = False

# Configure logging
logger = logging.getLogger(__name__)
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)


class ISNETrainingOrchestrator:
    """
    Orchestrates the complete ISNE model training workflow.
    
    This class handles the process of taking processed documents from the document
    pipeline, constructing a graph representation, training the ISNE model on this
    graph, evaluating model performance, and persisting the trained model.
    
    Attributes:
        input_dir: Directory containing processed documents from the pipeline
        output_dir: Directory for saving training artifacts
        model_output_dir: Directory for saving trained models
        config: Configuration settings for training
        device: Device to use for training (CPU or GPU)
    """
    
    def __init__(
        self,
        documents: Optional[List[Dict[str, Any]]] = None,
        input_dir: Optional[Union[str, Path]] = None,
        output_dir: Optional[Union[str, Path]] = None,
        model_output_dir: Optional[Union[str, Path]] = None,
        config_override: Optional[Dict[str, Any]] = None,
        device: Optional[str] = None
    ):
        """
        Initialize the ISNE training orchestrator.
        
        Args:
            documents: Optional list of processed document objects with embeddings (in-memory).
                      If provided, input_dir is ignored and documents are used directly.
            input_dir: Directory containing processed documents (from pipeline_multiprocess_test).
                       Used only if documents parameter is None.
            output_dir: Directory for storing training artifacts and results.
                         If None, uses the directory from config.
            model_output_dir: Directory for saving trained models.
                              If None, uses the directory from config.
            config_override: Optional dictionary to override default configuration settings
            device: Training device (e.g., "cpu", "cuda:0"); if None, uses config setting
        """
        # Store the documents passed directly in memory
        self.processed_documents = documents
        # Load configuration first so we can use directory defaults
        self.config = self._load_config()
        
        # Apply configuration overrides if provided
        if config_override:
            self._apply_config_override(config_override)
            
        # Get directory configuration
        dir_config = self.config.get("isne", {}).get("directories", {})
        
        # Set directory paths, using config values as defaults
        if input_dir is None:
            default_input = dir_config.get("input_dir", "./test-output/pipeline-mp-test")
            self.input_dir = Path(default_input)
        else:
            self.input_dir = Path(input_dir) if isinstance(input_dir, str) else input_dir
        
        if output_dir is None:
            default_output = dir_config.get("output_dir", "./test-output/isne-training")
            self.output_dir = Path(default_output)
        else:
            self.output_dir = Path(output_dir) if isinstance(output_dir, str) else output_dir
        
        if model_output_dir is None:
            default_model_dir = dir_config.get("model_dir", "./models/isne")
            self.model_output_dir = Path(default_model_dir)
        else:
            self.model_output_dir = Path(model_output_dir) if isinstance(model_output_dir, str) else model_output_dir
            
        # Set device based on configuration or explicit parameter
        if device is not None:
            self.device = device
        else:
            # Check global CPU/GPU execution settings first
            if "gpu_execution" in self.config and self.config["gpu_execution"].get("enabled", False):
                # Use GPU settings
                self.device = self.config["gpu_execution"].get("isne", {}).get("device", "cuda:0")
                logger.info(f"Using GPU execution mode with device: {self.device}")
            elif "cpu_execution" in self.config and self.config["cpu_execution"].get("enabled", False):
                # Use CPU settings
                self.device = "cpu"
                logger.info("Using CPU execution mode")
            else:
                # Fall back to ISNE-specific device setting
                device_config = self.config.get("isne", {}).get("training", {}).get("device", "cpu")
                self.device = device_config
                logger.info(f"Using ISNE-specific device setting: {self.device}")
        
        # Create directories if they don't exist
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.model_output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"ISNE Training Orchestrator initialized:")
        if self.processed_documents:
            logger.info(f"  Using {len(self.processed_documents)} documents provided directly in memory")
        elif self.input_dir:
            logger.info(f"  Input directory: {self.input_dir}")
        logger.info(f"  Output directory: {self.output_dir}")
        logger.info(f"  Model output directory: {self.model_output_dir}")
        logger.info(f"  Training device: {self.device}")
        
        # Initialize training metrics
        self.training_metrics = {
            "start_time": None,
            "end_time": None,
            "duration": None,
            "epochs": 0,
            "losses": [],
            "validation_metrics": []
        }
        
        # Initialize trainer
        self.trainer = None
    
    def _load_config(self) -> ISNEConfig:
        """
        Load configuration settings for ISNE training from the training pipeline config.
        
        Returns:
            Configuration dictionary containing ISNE settings
        """
        # Try to use config_loader if available
        if CONFIG_LOADER_AVAILABLE:
            try:
                # Load the unified training pipeline configuration
                config = load_pipeline_config(pipeline_type="training")
                logger.info("Loaded configuration using config_loader")
                
                # Check if ISNE configuration exists in the loaded config
                if "isne" not in config:
                    logger.warning("ISNE configuration not found in pipeline config, using defaults")
                    config["isne"] = {}
                    
                # Ensure all required sections exist
                for section in ["model", "training", "graph", "directories"]:
                    if section not in config["isne"]:
                        config["isne"][section] = {}
                
                # Set default values for model configuration if not present
                model_config = config["isne"]["model"]
                model_config.setdefault("embedding_dim", 768)
                model_config.setdefault("hidden_dim", 256)
                model_config.setdefault("output_dim", 768)
                model_config.setdefault("num_layers", 2)
                model_config.setdefault("num_heads", 8)
                model_config.setdefault("dropout", 0.1)
                model_config.setdefault("activation", "elu")
                model_config.setdefault("add_self_loops", True)
                
                # Set default values for training configuration if not present
                training_config = config["isne"]["training"]
                training_config.setdefault("learning_rate", 0.001)
                training_config.setdefault("weight_decay", 1e-5)
                training_config.setdefault("epochs", 50)
                training_config.setdefault("batch_size", 32)
                training_config.setdefault("num_hops", 1)
                training_config.setdefault("neighbor_size", 10)
                training_config.setdefault("eval_interval", 5)
                training_config.setdefault("early_stopping_patience", 10)
                training_config.setdefault("checkpoint_interval", 5)
                training_config.setdefault("device", "cpu")
                training_config.setdefault("lambda_feat", 1.0)
                training_config.setdefault("lambda_struct", 1.0)
                training_config.setdefault("lambda_contrast", 0.5)
                
                # Set default values for graph construction if not present
                graph_config = config["isne"]["graph"]
                graph_config.setdefault("similarity_threshold", 0.7)
                graph_config.setdefault("max_neighbors", 5)
                graph_config.setdefault("sequential_weight", 0.9)
                graph_config.setdefault("similarity_weight", 0.7)
                graph_config.setdefault("window_size", 3)
                
                # Set default values for directories if not present
                dir_config = config["isne"]["directories"]
                dir_config.setdefault("data_dir", "./data/isne")
                dir_config.setdefault("input_dir", "./test-output/pipeline-mp-test")
                dir_config.setdefault("output_dir", "./test-output/isne-training")
                dir_config.setdefault("model_dir", "./models/isne")
                
                return config
            except Exception as e:
                logger.warning(f"Error loading configuration with config_loader: {e}")
                logger.warning("Falling back to default configuration")
        
        # Fall back to default configuration
        logger.info("Using default ISNE training configuration")
        return {
            "isne": {
                "model": {
                    "embedding_dim": 768,
                    "hidden_dim": 256,
                    "output_dim": 768,
                    "num_layers": 2,
                    "num_heads": 8,
                    "dropout": 0.1,
                    "activation": "elu",
                    "add_self_loops": True
                },
                "training": {
                    "learning_rate": 0.001,
                    "weight_decay": 1e-5,
                    "epochs": 50,
                    "batch_size": 32,
                    "num_hops": 1,
                    "neighbor_size": 10,
                    "eval_interval": 5,
                    "early_stopping_patience": 10,
                    "checkpoint_interval": 5,
                    "device": "cpu",
                    "lambda_feat": 1.0,
                    "lambda_struct": 1.0,
                    "lambda_contrast": 0.5
                },
                "graph": {
                    "similarity_threshold": 0.7,
                    "max_neighbors": 5,
                    "sequential_weight": 0.9,
                    "similarity_weight": 0.7,
                    "window_size": 3
                },
                "directories": {
                    "data_dir": "./data/isne",
                    "input_dir": "./test-output/pipeline-mp-test",
                    "output_dir": "./test-output/isne-training",
                    "model_dir": "./models/isne"
                }
            }
        }
    
    def _apply_config_override(self, override: Dict[str, Any]) -> None:
        """
        Apply configuration overrides to the current configuration.
        
        Args:
            override: Dictionary with configuration overrides
        """
        # Handle fully nested structure (isne.section.parameter)
        if "isne" in override:
            isne_override = override["isne"]
            for section_name, section_values in isne_override.items():
                # Create section if it doesn't exist
                if section_name not in self.config["isne"]:
                    self.config["isne"][section_name] = {}
                    
                # Apply section overrides
                for key, value in section_values.items():
                    self.config["isne"][section_name][key] = value
                    
            logger.info(f"Applied nested configuration overrides to sections: {list(isne_override.keys())}")
        
        # Handle partially nested overrides (section.parameter) - these go to isne.section.parameter
        elif any(section in override for section in ["model", "training", "graph", "directories"]):
            for section_name in ["model", "training", "graph", "directories"]:
                if section_name in override:
                    # Create section if it doesn't exist
                    if section_name not in self.config["isne"]:
                        self.config["isne"][section_name] = {}
                        
                    # Apply overrides to this section
                    for key, value in override[section_name].items():
                        self.config["isne"][section_name][key] = value
                        
            logger.info("Applied section-level configuration overrides")
        
        # Handle flat overrides (parameter) - these go to isne.training.parameter for backward compatibility
        else:
            for key, value in override.items():
                self.config["isne"]["training"][key] = value
            
            logger.info("Applied flat configuration overrides to training section")
        
        logger.info("Configuration override completed")
    
    def _load_processed_documents(self) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        Load processed documents with embeddings from in-memory objects or from the input directory.
        
        Returns:
            Tuple of (processed_documents, pipeline_stats)
        """
        # If documents were provided directly in memory, use those
        if self.processed_documents is not None:
            logger.info(f"Using {len(self.processed_documents)} documents provided directly in memory")
            pipeline_stats = {
                "total_files": len(self.processed_documents),
                "processing_time": {},
                "file_details": {}
            }
            return self.processed_documents, pipeline_stats
        
        # Otherwise load from files (fallback method)
        # Check if input directory exists
        if not self.input_dir or not self.input_dir.exists():
            raise ValueError(f"Input directory not found: {self.input_dir}")
        
        # Define paths to load
        isne_input_path = self.input_dir / "isne_input_sample.json"
        stats_path = self.input_dir / "pipeline_stats.json"
        
        # Validate input files existence
        if not isne_input_path.exists():
            raise ValueError(f"ISNE input file not found: {isne_input_path}")
        
        # Load processed documents
        with open(isne_input_path, "r") as f:
            processed_documents = json.load(f)
        
        # Load pipeline stats if available, or create empty stats object
        if not stats_path.exists():
            logger.warning(f"Pipeline stats file not found, using empty stats: {stats_path}")
            pipeline_stats = {
                "total_files": len(processed_documents),
                "processing_time": {},
                "file_details": {}
            }
        else:
            with open(stats_path, "r") as f:
                pipeline_stats = json.load(f)
        
        logger.info(f"Loaded {len(processed_documents)} processed documents from {isne_input_path}")
        
        return processed_documents, pipeline_stats
    
    def _construct_graph(self, documents: List[Dict[str, Any]]) -> Data:
        """
        Construct a graph from the processed documents based on configuration.
        
        Args:
            documents: List of processed documents with chunks and embeddings
            
        Returns:
            PyTorch Geometric Data object representing the document graph
        """
        logger.info("Constructing document graph for ISNE training...")
        
        # Get graph construction parameters from typed config
        graph_config: ISNEGraphConfig = self.config.get("isne", {}).get("graph", {})
        similarity_threshold = graph_config.get("similarity_threshold", 0.7)
        max_neighbors = graph_config.get("max_neighbors", 5)
        sequential_weight = graph_config.get("sequential_weight", 0.9)
        similarity_weight = graph_config.get("similarity_weight", 0.7)
        window_size = graph_config.get("window_size", 3)
        
        # Collect chunk embeddings and IDs
        node_features = []
        node_ids = []
        id_to_index = {}  # Maps chunk ID to its index in the node list
        
        # Create nodes for each chunk with embeddings
        for doc in documents:
            for chunk in doc.get("chunks", []):
                # Handle chunks without embeddings
                if not chunk.get("embedding"):
                    # For testing purposes: generate random embeddings when none are found
                    if self.config.get("isne", {}).get("model", {}).get("embedding_dim"):
                        embedding_dim = self.config["isne"]["model"]["embedding_dim"]
                        # Use deterministic seed based on chunk id for reproducibility
                        seed = int(hash(chunk["id"]) % 10000000)
                        np.random.seed(seed)
                        random_embedding = np.random.normal(0, 0.1, embedding_dim).tolist()
                        chunk["embedding"] = random_embedding
                        logger.info(f"Generated random embedding for chunk {chunk['id']} (test mode)")
                    else:
                        # Skip if we can't determine embedding dimension
                        logger.warning(f"Skipping chunk without embedding: {chunk['id']}")
                        continue
                
                # Add to node list
                chunk_id = chunk["id"]
                node_ids.append(chunk_id)
                id_to_index[chunk_id] = len(node_ids) - 1
                
                # Convert embedding to tensor if it's a list
                embedding = chunk["embedding"]
                if isinstance(embedding, list):
                    embedding = torch.tensor(embedding, dtype=torch.float)
                
                node_features.append(embedding)
        
        # Check if we have nodes
        if not node_features:
            raise ValueError("No valid chunks with embeddings found in processed documents")
        
        # Stack node features into a single tensor
        node_features = torch.stack(node_features)
        
        # Create edges based on document structure and semantic similarity
        edge_list = []
        edge_weights = []
        
        # Helper function to add an edge
        def add_edge(src_id, dst_id, weight=1.0):
            if src_id in id_to_index and dst_id in id_to_index:
                src_idx = id_to_index[src_id]
                dst_idx = id_to_index[dst_id]
                
                # Avoid self-loops for now (we'll add them explicitly later if needed)
                if src_idx != dst_idx:
                    edge_list.append([src_idx, dst_idx])
                    edge_weights.append(weight)
        
        # Add sequential edges within documents
        for doc in documents:
            chunks = doc.get("chunks", [])
            for i in range(len(chunks) - 1):
                # Sequential connection with weight from config
                add_edge(chunks[i]["id"], chunks[i+1]["id"], weight=sequential_weight)
        
        # Add edges for semantically similar chunks based on overlap context
        for doc in documents:
            chunks = doc.get("chunks", [])
            for i, chunk in enumerate(chunks):
                if "overlap_context" in chunk:
                    # If there are previous/next chunks indicated in overlap context, connect them
                    if "position" in chunk["overlap_context"] and "total" in chunk["overlap_context"]:
                        pos = chunk["overlap_context"]["position"]
                        total = chunk["overlap_context"]["total"]
                        
                        # Connect to neighboring chunks within a window defined by config
                        for j in range(max(0, i-window_size), min(len(chunks), i+window_size+1)):
                            if i != j:  # Skip self
                                # Weight based on proximity and config similarity weight
                                proximity_factor = 1.0 - (abs(i-j) / (window_size+1))
                                edge_weight = similarity_weight * proximity_factor
                                add_edge(chunk["id"], chunks[j]["id"], weight=edge_weight)
        
        # Convert edge list to tensor
        if not edge_list:
            raise ValueError("No edges created in the document graph")
        
        edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()
        edge_attr = torch.tensor(edge_weights, dtype=torch.float)
        
        # Make graph undirected
        edge_index, edge_attr = to_undirected(edge_index, edge_attr)
        
        # Create PyTorch Geometric Data object
        graph_data = Data(
            x=node_features,
            edge_index=edge_index,
            edge_attr=edge_attr,
            num_nodes=len(node_ids)
        )
        
        logger.info(f"Created graph with {graph_data.num_nodes} nodes and {graph_data.num_edges} edges")
        
        return graph_data
    
    def _prepare_trainer(self) -> ISNETrainer:
        """
        Initialize and prepare the ISNE trainer using configuration settings.
        
        Returns:
            Initialized ISNETrainer instance
        """
        # Get model and training parameters from typed config
        model_config: ISNEModelConfig = self.config["isne"]["model"]
        train_config: ISNETrainingConfig = self.config["isne"]["training"]
        
        # Convert config values to proper types to avoid string conversion issues
        # Initialize trainer with settings from configuration
        trainer = ISNETrainer(
            embedding_dim=int(model_config["embedding_dim"]),
            hidden_dim=int(model_config["hidden_dim"]),
            output_dim=int(model_config["output_dim"]),
            num_layers=int(model_config["num_layers"]),
            num_heads=int(model_config["num_heads"]),
            dropout=float(model_config["dropout"]),
            learning_rate=float(train_config["learning_rate"]),
            weight_decay=float(train_config["weight_decay"]),
            lambda_feat=float(train_config.get("lambda_feat", 1.0)),
            lambda_struct=float(train_config.get("lambda_struct", 1.0)),
            lambda_contrast=float(train_config.get("lambda_contrast", 0.5)),
            device=self.device
        )
        
        # Prepare the model, losses, and optimizer
        trainer.prepare_model()
        
        logger.info("Initialized ISNE trainer with configuration parameters")
        
        return trainer
    
    def train(self) -> Dict[str, Any]:
        """
        Execute the complete ISNE training workflow.
        
        Returns:
            Dictionary with training results and metrics
        """
        logger.info("Starting ISNE training workflow...")
        
        # Track overall training time
        self.training_metrics["start_time"] = time.time()
        
        # Load processed documents (either from memory or from files)
        documents, pipeline_stats = self._load_processed_documents()
        
        # Construct the document graph
        graph_data = self._construct_graph(documents)
        
        # Prepare trainer
        self.trainer = self._prepare_trainer()
        
        # Get training parameters from typed config
        train_config: ISNETrainingConfig = self.config["isne"]["training"]
        
        logger.info(f"Training ISNE model with {train_config['epochs']} epochs on {self.device}...")
        
        # Train the model
        train_stats = self.trainer.train(
            features=graph_data.x,
            edge_index=graph_data.edge_index,
            epochs=train_config["epochs"],
            batch_size=train_config["batch_size"],
            num_hops=train_config["num_hops"],
            neighbor_size=train_config["neighbor_size"],
            eval_interval=train_config["eval_interval"],
            early_stopping_patience=train_config["early_stopping_patience"],
            verbose=True
        )
        
        # Update training metrics
        self.training_metrics["end_time"] = time.time()
        self.training_metrics["duration"] = self.training_metrics["end_time"] - self.training_metrics["start_time"]
        self.training_metrics["epochs"] = train_stats["epochs"]
        self.training_metrics["losses"] = {
            "total_loss": train_stats["total_loss"],
            "feature_loss": train_stats["feature_loss"],
            "structural_loss": train_stats["structural_loss"],
            "contrastive_loss": train_stats["contrastive_loss"]
        }
        
        # Get model naming configuration
        model_config = self.config.get("isne", {}).get("model", {})
        name_prefix = model_config.get("name_prefix", "isne_model")
        version = model_config.get("version", "v1")
        use_timestamp = model_config.get("use_timestamp", True)
        keep_previous = model_config.get("keep_previous_versions", True)
        
        # Create model filename
        if use_timestamp:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            model_filename = f"{name_prefix}_{version}_{timestamp}.pt"
        else:
            model_filename = f"{name_prefix}_{version}.pt"
            
        # Save the model
        model_path = self.model_output_dir / model_filename
        self.trainer.save_model(model_path)
        logger.info(f"Model saved to: {model_path}")
        
        # Save a symlink to the latest model
        latest_model_path = self.model_output_dir / f"{name_prefix}_latest.pt"
        if latest_model_path.exists():
            latest_model_path.unlink()
        latest_model_path.symlink_to(model_filename)
        
        # Generate enhanced embeddings for visualization
        enhanced_embeddings = self.trainer.get_embeddings(graph_data.x, graph_data.edge_index)
        
        # Save training results
        results_path = self.output_dir / "training_results.json"
        with open(results_path, "w") as f:
            json.dump({
                "training_metrics": self.training_metrics,
                "config": self.config["isne"]["training"],
                "model_path": str(model_path),
                "pipeline_stats": pipeline_stats
            }, f, indent=2)
        
        logger.info(f"ISNE training completed in {self.training_metrics['duration']:.2f} seconds")
        logger.info(f"Trained for {self.training_metrics['epochs']} epochs")
        logger.info(f"Model saved to {model_path}")
        logger.info(f"Training results saved to {results_path}")
        
        return self.training_metrics
    
    def load_model(self, model_path: Optional[Union[str, Path]] = None) -> ISNEModel:
        """
        Load a trained ISNE model.
        
        Args:
            model_path: Path to the model file (defaults to latest model)
            
        Returns:
            Loaded ISNEModel instance
        """
        # Use the specified path or default to the latest model
        if model_path is None:
            model_path = self.model_output_dir / "isne_model_latest.pt"
        else:
            model_path = Path(model_path) if isinstance(model_path, str) else model_path
        
        # Check if the model file exists
        if not model_path.exists():
            raise FileNotFoundError(f"Model file not found at {model_path}")
        
        # Initialize trainer if not already done
        if self.trainer is None:
            self.trainer = self._prepare_trainer()
        
        # Load the model
        self.trainer.load_model(model_path)
        
        logger.info(f"Loaded ISNE model from {model_path}")
        
        return self.trainer.model


def main():
    """
    Main function for running the ISNE training pipeline from command line.
    """
    parser = argparse.ArgumentParser(description="ISNE Training Orchestrator")
    parser.add_argument('--input-dir', type=str, default=None,
                        help='Directory containing processed documents from the pipeline')
    parser.add_argument('--output-dir', type=str, default=None,
                        help='Directory for storing training artifacts and results')
    parser.add_argument('--model-output-dir', type=str, default=None,
                        help='Directory for saving trained models')
    parser.add_argument('--epochs', type=int, default=None,
                        help='Number of training epochs')
    parser.add_argument('--learning-rate', type=float, default=None,
                        help='Learning rate for training')
    parser.add_argument('--hidden-dim', type=int, default=None,
                        help='Hidden dimension size for ISNE model')
    parser.add_argument('--device', type=str, default=None,
                        help='Device to use for training (e.g., "cpu", "cuda:0")')
    
    args = parser.parse_args()
    
    # Prepare configuration override
    config_override = {"training": {}}
    if args.epochs is not None:
        config_override["training"]["epochs"] = args.epochs
    if args.learning_rate is not None:
        config_override["training"]["learning_rate"] = args.learning_rate
    if args.hidden_dim is not None:
        config_override["training"]["hidden_dim"] = args.hidden_dim
    
    # Initialize orchestrator with file-based approach for CLI usage
    # (documents parameter is None, so it will load from input_dir)
    orchestrator = ISNETrainingOrchestrator(
        documents=None,  # When called from CLI, use file-based approach
        input_dir=args.input_dir,
        output_dir=args.output_dir,
        model_output_dir=args.model_output_dir,
        config_override=config_override if config_override["training"] else None,
        device=args.device
    )
    
    # Run training
    orchestrator.train()


if __name__ == "__main__":
    main()
