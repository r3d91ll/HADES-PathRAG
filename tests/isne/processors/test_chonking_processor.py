"""
Tests for the Chonky semantic chunking processor.

This module provides tests for the ChonkyProcessor which uses the Chonky neural
chunking approach for semantic document chunking.
"""

import pytest
from unittest.mock import MagicMock, patch
import uuid
from datetime import datetime

from src.isne.types.models import IngestDocument, IngestDataset, DocumentRelation, RelationType
from src.isne.processors.chonking_processor import ChonkyProcessor
from src.isne.processors.base_processor import ProcessorConfig, ProcessorResult


@pytest.fixture
def mock_paragraph_splitter():
    """Create a mock ParagraphSplitter."""
    with patch('src.isne.processors.chonking_processor.ParagraphSplitter') as mock_splitter_class:
        mock_splitter = MagicMock()
        # Configure the mock to return semantic chunks
        mock_splitter.__call__.return_value = [
            "This is the first semantic chunk of text that Chonky identified.",
            "Here is the second chunk with different semantic content.",
            "Finally, the third chunk talks about something else entirely."
        ]
        mock_splitter_class.return_value = mock_splitter
        yield mock_splitter


@pytest.fixture
def sample_documents():
    """Create sample IngestDocument objects for testing."""
    docs = []
    # Create a text document
    text_doc = IngestDocument(
        id="doc1",
        content="This is a sample document with multiple paragraphs. It contains various topics and ideas. "
               "Chonky should be able to split this into meaningful semantic chunks. "
               "Each chunk should represent a coherent thought or idea. "
               "This allows for better retrieval in RAG systems.",
        source="test",
        document_type="text",
        title="Sample Document",
        created_at=datetime.now()
    )
    docs.append(text_doc)
    
    # Create a code document
    code_doc = IngestDocument(
        id="doc2",
        content="def hello_world():\n    print('Hello, world!')\n\n"
               "class SampleClass:\n    def __init__(self):\n        self.value = 42\n"
               "    def get_value(self):\n        return self.value",
        source="test.py",
        document_type="code",
        title="Sample Code",
        created_at=datetime.now()
    )
    docs.append(code_doc)
    
    return docs


class TestChonkyProcessor:
    """Test suite for ChonkyProcessor."""
    
    def test_init(self, mock_paragraph_splitter):
        """Test the initialization of ChonkyProcessor."""
        processor = ChonkyProcessor(
            model_id="mirth/chonky_distilbert_uncased_1",
            device="cpu"
        )
        assert processor.model_id == "mirth/chonky_distilbert_uncased_1"
        assert processor.device == "cpu"
        assert processor.preserve_metadata is True
        assert processor.create_relationships is True
        assert processor.text_only is True
        assert processor.splitter is not None
    
    def test_process_text_documents(self, mock_paragraph_splitter, sample_documents):
        """Test processing text documents with ChonkyProcessor."""
        processor = ChonkyProcessor()
        
        # Process only the text document from sample_documents
        text_doc = [doc for doc in sample_documents if doc.document_type == "text"][0]
        result = processor.process([text_doc])
        
        # Verify the result
        assert len(result.documents) == 4  # Original + 3 chunks
        
        # Check that the original document has been updated with chunk info
        original_doc = [doc for doc in result.documents if doc.id == text_doc.id][0]
        assert "chunk_count" in original_doc.metadata
        assert original_doc.metadata["chunk_count"] == 3
        assert original_doc.metadata["chunking_strategy"] == "chonky_semantic"
        
        # Check the chunks
        chunks = [doc for doc in result.documents if doc.id != text_doc.id]
        assert len(chunks) == 3
        
        # Check relationships
        assert len(result.relations) == 5  # 3 parent-child + 2 sequential
        
        # Verify parent-child relationships
        parent_child_relations = [
            rel for rel in result.relations 
            if rel.relation_type == RelationType.CONTAINS
        ]
        assert len(parent_child_relations) == 3
        
        # Verify sequential relationships
        sequential_relations = [
            rel for rel in result.relations 
            if rel.relation_type == RelationType.FOLLOWS
        ]
        assert len(sequential_relations) == 2
    
    def test_skip_code_documents(self, mock_paragraph_splitter, sample_documents):
        """Test that code documents are skipped when text_only is True."""
        processor = ChonkyProcessor(text_only=True)
        
        # Process all documents
        result = processor.process(sample_documents)
        
        # Verify that code document was not chunked
        processed_ids = [doc.id for doc in result.documents]
        original_ids = [doc.id for doc in sample_documents]
        
        # We should have the original text document, the original code document,
        # and the chunks from the text document
        assert len(processed_ids) > len(original_ids)
        
        # Code document should be in the result unchanged
        code_doc = [doc for doc in result.documents if doc.document_type == "code"][0]
        assert code_doc.id == "doc2"
        assert "chunk_count" not in code_doc.metadata
    
    def test_process_all_documents(self, mock_paragraph_splitter, sample_documents):
        """Test processing all documents including code when text_only is False."""
        processor = ChonkyProcessor(text_only=False)
        
        # Process all documents
        result = processor.process(sample_documents)
        
        # Verify that both documents were chunked
        # We should have 2 original docs + 6 chunks (3 per document)
        assert len(result.documents) == 8
        
        # Both documents should have chunk info
        for doc_id in ["doc1", "doc2"]:
            original_doc = [doc for doc in result.documents if doc.id == doc_id][0]
            assert "chunk_count" in original_doc.metadata
            assert original_doc.metadata["chunk_count"] == 3
    
    def test_splitter_initialization_failure(self):
        """Test handling of initialization failure."""
        with patch('src.isne.processors.chonking_processor.ParagraphSplitter', 
                  side_effect=Exception("Failed to initialize")):
            processor = ChonkyProcessor()
            assert processor.splitter is None
            
            # Process should handle the missing splitter gracefully
            result = processor.process([
                IngestDocument(id="test", content="Test content", source="test")
            ])
            
            assert len(result.errors) == 1
            assert "Chonky splitter not initialized" in result.errors[0]["error"]
    
    def test_empty_documents(self, mock_paragraph_splitter):
        """Test handling of empty documents."""
        processor = ChonkyProcessor()
        
        empty_doc = IngestDocument(
            id="empty",
            content="",
            source="test"
        )
        
        result = processor.process([empty_doc])
        
        # The empty document should be returned unchanged
        assert len(result.documents) == 1
        assert result.documents[0].id == "empty"
    
    def test_create_semantic_chunks_error(self, mock_paragraph_splitter):
        """Test error handling in _create_semantic_chunks."""
        processor = ChonkyProcessor()
        
        # Make the splitter raise an exception
        processor.splitter.__call__.side_effect = Exception("Chunking error")
        
        doc = IngestDocument(
            id="error_doc",
            content="This will cause an error",
            source="test"
        )
        
        # This should return an empty list, not raise an exception
        chunks = processor._create_semantic_chunks(doc)
        assert len(chunks) == 0


if __name__ == "__main__":
    pytest.main()
