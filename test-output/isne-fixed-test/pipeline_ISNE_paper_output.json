{
  "id": "pdf_0dbab603_ISNE_paper.pdf",
  "metadata": {
    "language": "en",
    "format": "markdown",
    "content_type": "text",
    "file_size": null,
    "line_count": null,
    "char_count": null,
    "has_errors": false,
    "page_count": 16,
    "file_path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
    "doc_type": "academic_pdf",
    "title": "ORIGINAL ARTICLE",
    "authors": [
      "UNK"
    ],
    "date_published": "2024",
    "publisher": "arXiv",
    "source": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf"
  },
  "source": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
  "format": "pdf",
  "content_type": "text",
  "chunks": [
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p0",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": "## ORIGINAL ARTICLE\n\n## Unsupervised Graph Representation Learning with Inductive Shallow Node Embedding\n\n<!-- image -->\n\nRich\u00e1rd Kiss 1 \u00b7 G\u00e1bor Sz\u02dd ucs 1\n\nReceived: 26 March 2024 / Accepted: 21 June 2024 / Published online: 12 July 2024 \u00a9The Author(s) 2024\n\n## Abstract\n\nNetwork science has witnessed a surge in popularity, driven by the transformative power of node representation learning for diverse applications like social network analysis and biological modeling. While shallow embedding algorithms excel at capturing network structure, they face a critical limitation-failing to generalize to unseen nodes. This paper addresses this challenge by introducing Inductive Shallow Node Embedding-as a main contribution-pioneering a novel approach that extends shallow embeddings to the realm of inductive learning. It has a novel encoder architecture that captures the local neighborhood structure of each node, enabling effective generalization to unseen nodes. In the generalization, robustness is essential to avoid degradation of performance arising from noise in the dataset. It has been theoretically proven that the covariance of the additive noise term in the proposed model is inversely proportional to the cardinality of a node's neighbors. Another contribution is a mathematical lower bound to quantify the robustness of node embeddings, confirming its advantage over traditional shallow embedding methods, particularly in the presence of parameter noise. The proposed method demonstrably excels in dynamic networks, consistently achieving over 90% performance on previously unseen nodes compared to nodes encountered during training on various benchmarks. The empirical evaluation concludes that our method outperforms competing methods on the vast majority of datasets in both transductive and inductive tasks.\n\nKeywords Node embedding \u00b7 Graph representation learning \u00b7 Inductive learning \u00b7 Dynamic networks \u00b7 Node2Vec \u00b7 GraphSAGE\n\n## Introduction\n\nNetwork science has experienced a surge in interest, fueled by the transformative potential of node representation learning [1] for diverse applications like bioinformatics [2, 3], chemoinformatics [4, 5], recommendation systems [6-8], social network analysis [9], and more. At the heart of this field lies the ability to capture meaningful representations of nodes within complex networks [10].\n\nstructural information within networks. However, they are inherently transductive, limiting their ability to generalize to unseen nodes, a significant drawback in dynamic networks or scenarios with incomplete initial knowledge.\n\nShallow node embedding techniques, exemplified by the popular Node2Vec [11] algorithm, play a crucial role in this endeavor [12, 13]. These methods excel at capturing the\n\nB\n\nG\u00c6bor Sz\u02dd ucs szucs@tmit.bme.hu\n\nRich\u00c6rd Kiss richard.kiss@edu.bme.hu\n\n1 Department of Telecommunications and Artificial Intelligence, Budapest University of Technology and Economics, M\u02dd uegyetem rkp. 3., Budapest 1111, Hungary\n\nGraphSAGE (Graph SAmple and aggreGatE) [14], a pioneering work in inductive node representation learning, addresses this limitation by employing Message Passing Neural Networks (MPNN). However, the performance of MPNNs is heavily reliant on the quality and availability of node features. In scenarios where node features are missing, unreliable, or not informative for the task at hand, GraphSAGE's effectiveness can be diminished.\n\nOur research addresses a notable gap in the field: solving inductive tasks in graph analysis where GraphSAGE struggles, particularly because it relies heavily on node features for effective learning. We sought to develop a solution capable of learning without these features and making predictions on new nodes, achieving true inductive capabilities, unlike the transductive nature of Node2Vec. Additionally, our study aims to fill a void in the literature by providing a robust theo-\n\n<!-- image -->\n\nretical analysis of the effects of noise, which has been largely overlooked in previous research. The aim of this work was to bridge these gaps and advance the understanding and application of graph-based machine learning.\n\nTo address these limitations, a novel approach, so called Inductive Shallow Node Embedding (ISNE) was introduced, that extends shallow node embeddings to inductive learning. Unlike traditional methods, ISNE employs a unique encoder architecture that captures the local neighborhood structure of each node, enabling it to generalize effectively to unseen nodes. This makes ISNE particularly valuable for dynamic networks where the network structure evolves over time.\n\nThe main contributions of this work are as follows:\n\n- \u00b7 Novel encoder design : The introduction of a new encoder that captures local neighborhood structures for inductive learning is a significant advancement. This encoder moves beyond the limitations of traditional lookup tables and message-passing frameworks.\n- \u00b7 Inductive learning for shallow embeddings : Extending shallow embeddings to inductive learning creates a new category of algorithms that combine the simplicity and effectiveness of shallow methods with the generalizability of inductive approaches.\n- \u00b7 Theoretical insights into robustness : Providing a mathematical proof of the robustness of the embeddings under parameter noise offers a new understanding of how embeddings can remain reliable in noisy and dynamic environments. This can influence future research on the robustness of machine learning models.\n- \u00b7 Hybrid embedding and attribute utilization : Demonstrating the effective combination of structural embeddings with node attributes creates a hybrid approach that leverages the strengths of both. This knowledge can inform the design of future algorithms that need to balance structural and attribute information.\n- \u00b7 Comparative analysis on new nodes : The ability of ISNE to adapt to dynamic networks is demonstrated, consistently achieving over 90% performance on previously unseen nodes compared to nodes encountered during training across various benchmarks.\n- \u00b7 Comprehensive empirical evaluation : Extensive experiments are conducted on multiple datasets to validate ISNE's performance in both transductive and inductive settings, showcasing its superiority over traditional methods and state-of-the-art inductive algorithms like GraphSAGE.\n\nBy addressing the inherent limitations of traditional shallow embedding methods and advancing the capabilities of inductive learning, ISNE represents a significant step forward in the field of network science. This work lays the foundation for more robust and adaptable node representation learning\n\n<!-- image -->\n\ntechniques, capable of handling the complexities of dynamic and evolving networks.\n\nIn the remainder of this paper, the related works are reviewed in Sect. 'Related Work', the node representation learning framework with shallow encoders is presented in Sect. 'Noderepresentation learning with shallow encoders. A novel perspective that addresses the limitations and a method called Inductive Shallow Node Embedding are introduced in Sect. 'Inductive Shallow Node Embedding'. A theoretical analysis of the robustness of the proposed method is provided in Sect. 'Theoretical analysis of robustness'. In Sect. 'Empirical results', the empirical performance of the proposed method on both transductive and inductive tasks is evaluated, demonstrating promising results. Finally, the last section concludes the paper with remarks on future research directions.\n\n## Related work\n\n## Shallow embedding methods\n\nTraditional shallow embedding methods excel at capturing network structure through techniques like random walks (DeepWalk [15], Node2Vec [11]) or preserving proximity relationships (LINE-Large-scale Information Network Embedding [16], NetMF-NETwork embedding as Matrix Factorization [17], GraRep-GRAph REPresentations [18], PTE-Predictive Text Embedding [19]). However, these methods rely on a lookup table encoder architecture, limiting their ability to generalize to unseen nodes not encountered during training. Applications include recommendation systems [10, 20], financial fraud detection [21], learning text representations [19], and predicting miRNA-disease associations [22, 23].\n\n## Inductive node representation learning\n\nGraphSAGE [14] represents a significant advancement by introducing a message-passing framework for unsupervised inductive node representation learning. This approach allows GraphSAGE to effectively handle unseen nodes, making it particularly valuable for dynamic networks. GraphSAGE has been successfully applied to various tasks, including knowledge graph completion [24], recommendation systems [25], intrusion detection [26], prediction of molecular toxicity [27], financial portfolio optimization [28], and traffic speed forecasting [29]. However, GraphSAGE's effectiveness depends on the quality and availability of node attributes, which can be a limitation in certain scenarios.\n\nISNE builds upon these approaches by introducing a novel inductive encoder that captures network structure without relying on a static lookup table encoder or requiring\n\nhigh-quality node features. This enables ISNE to effectively generalize to unseen nodes in dynamic networks, even when node attributes are unavailable or unreliable.\n\n## Node representation learning with shallow encoders\n\n## Notation\n\nTo enhance clarity and understanding, consistent notation throughout the paper is established:\n\n## Variables\n\n- \u00b7 A : Matrix, A [ i , j ] denotes the element in the i-th row and j-th column\n- \u00b7 A 2 : Element-wise square of matrix A , A 2 [ i , j ] = ( A [ i , j ] ) 2\n- \u00b7 v n : Specific version of vector v (subscript denotes version)\n- \u00b7 1: Vector with all elements equal to 1\n- \u00b7 v /latticetop n : Transpose of vector v n\n- \u00b7 0: Vector with all elements equal to 0\n\n## Graph Properties\n\n- \u00b7 G = ( V , E ) : Graph with nodes V and undirected edges E \u2208 V \u00d7 V\n- \u00b7 | N i | : Number of neighbors for node i\n- \u00b7 N i : Set of neighboring nodes for node i\n- \u00b7 N i , j : Intersection of neighbors between nodes i and j\n- \u00b7 | N i , j | : Number of common neighbors of i and j\n\n## Embedding and Similarity\n\n- \u00b7 f : V \u2192 R D : Function mapping nodes to Ddimensional representations (encoder)\n- \u00b7 f ( ) i : Embedding vector of node i\n- \u00b7 \u02dc f ( ) i : Embeddingvector of node i with a noise augmented encoder function f\n- \u00b7 s f : V \u00d7 V \u2192 R : Node similarity function based on encoder f\n- \u00b7 s \u02dc f : V \u00d7 V \u2192 R : Node similarity function based on the noise augmented encoder function \u02dc f\n\n## The node representation learning framework\n\nNode representation learning aims to discover lowdimensional vector representations (embeddings) for nodes in a network. These embeddings capture the inherent structure and relationships within the network, allowing them to be readily utilized in various downstream tasks such as node classification, link prediction, and community detection.\n\nThe core concept of this learning process revolves around learning an encoder function with learnable weights. This function takes a node as input and maps it to its corresponding embedding in a low-dimensional space. Different algorithms employ distinct encoder functions, similarity metrics in the embedding space, and methods for defining node similarity within the graph.\n\nIdeally, this process leads to embeddings where geometric relationships between nodes in the low-dimensional space accurately reflect the structural relationships within the original network. Nodes that exhibit higher similarity within the network should be positioned closer together in the embedding space, and vice versa.\n\n## Shallow encoders\n\nShallow Encoder Algorithms originally represent nodes using a lookup table, which assigns a unique, pre-allocated embedding vector to each node. This function, denoted as f , essentially maps a node v in the network to its corresponding embedding vector in the low-dimensional space: f (v) = \u03b8v . Algorithms that utilize lookup table encoders suffer from two key limitations:\n\n- 1. Transductivity : Due to its reliance on a pre-defined lookup table, Node2Vec [11] is inherently transductive. This means the model cannot generalize to unseen nodes, which were not present during the training process. This limitation hinders its applicability in scenarios involving dynamic networks or tasks requiring predictions for new nodes.\n- 2. Static Embeddings : The learned representations generated by Node2Vec are static. Any changes to the network structure, such as adding or removing edges, do not trigger updates to the existing node embeddings. This lack of adaptability can be problematic in real-world networks that often exhibit dynamic changes.\n\n## Inductive shallow node embedding\n\nInductive Shallow Node Embedding (ISNE) offers a novel perspective that addresses the limitations explained in Sect. 'The node representation learning framework'. ISNE leverages a novel encoder function that overcomes the challenges associated with both unseen nodes and dynamic network structures. This novel design empowers ISNE to:\n\n- \u00b7 Generalize to unseen nodes : Unlike transductive methods, ISNE can effectively represent even nodes not present during training.\n\n- \u00b7 Adapt to dynamic networks : ISNE representations can adjust to changes in the network structure, making them suitable for evolving network scenarios.\n- \u00b7 Function independently of node attributes : ISNE embeddings are constructed solely based on the network structure, eliminating the dependency on potentially unreliable or unavailable attribute information.\n\nFurthermore, ISNE retains the flexibility to incorporate node attributes by simply concatenating them to the existing ISNEembeddings.Thisallowsuserstoleveragethestrengths of both network structure and node attributes, potentially leading to even more robust and informative representations. The following section delves deeper into the details of the proposed ISNE method, including its novel encoder function and its theoretical properties.\n\n## Methodology\n\nUnlike traditional shallow embedding methods that rely on lookup tables, the proposed Inductive Shallow Node Embedding (ISNE) method leverages a novel encoder function to construct node embeddings. This function operates based on the immediate neighbors of each node, as captured by the neighborhood set denoted by N v . The core equation for the ISNE encoder is presented as follows:\n\n<!-- formula-not-decoded -->\n\nIn this equation, h (v) represents the embedding vector of node v , and the summation iterates through all neighbors n within its neighborhood set. This design ensures that the embedding of a node is informed by the parameters of its immediate neighbors, effectively capturing the local network structure around each node.\n\nThis approach offers several key advantages:\n\n- \u00b7 Dynamic updates : Whenever a new edge is added to the network, the neighborhood set of affected nodes (i.e., N j for specific nodes j ) is updated accordingly. By recalculating h (v) for these nodes, the ISNE embeddings automatically reflect the latest network structure changes within their local neighborhoods.\n- \u00b7 Handling unseen nodes : The ISNE framework is capable of generating embeddings for previously unseen nodes, provided their connections are known. By incorporating these connections into the neighborhood set during the encoding process (i.e., adding them to N v for the unseen node), ISNE can effectively estimate their embeddings.\n- \u00b7 Inductive learning : This unique design empowers ISNE to perform inductive learning tasks. By relying solely on\n\n<!-- image -->\n\nthe network structure and generalizing from known information, the model can infer embeddings for unseen and modified data points, significantly expanding its applicability in dynamic network settings and demonstrating adaptability to evolving graph structures.\n\nIn essence, the ISNE encoder overcomes the limitations of lookup tables by enabling dynamic updates, handling unseen nodes, and facilitating inductive learning tasks, thereby establishing itself as a valuable tool for various network analysis applications.\n\n## Theoretical analysis of robustness\n\nThis section investigates the robustness of ISNE compared to the traditional lookup table encoder in the presence of parameter noise. This can be achieved by introducing zero-mean additive noise, denoted by zn , into the model parameters \u03b8 n . Each zn is independently and identically distributed (i.i.d.) following a common multivariate Gaussian distribution with zero mean and covariance matrix /Sigma1 . We denote the noisecorrupted versions of the lookup table and ISNE encoders as \u02dc f and \u02dc h , respectively:\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\nHere, /epsilon1 i = | 1 N i | \u2211 n \u2208 N i zn captures the additive noise introduced by the neighborhood aggregation in the ISNE encoder. Since /epsilon1 i is the sum of i.i.d. Gaussian random variables, it also follows a multivariate Gaussian distribution with zero mean. However, it's important to note that the noise vectors /epsilon1 n 1 and /epsilon1 n 2 for different nodes n 1 and n 2 might not be independent. This is because the aggregation in /epsilon1 i involves noise terms z from potentially overlapping neighborhoods, i.e., N n 1 \u2229 N n 2 = \u2205 .\n\n## Representation robustness\n\nCovariance analysis is a valuable tool for evaluating the robustness of node representations. This analysis focuses on the inherent noise level within the ISNE embeddings.\n\nTheorem 1 The covariance of the additive noise term /epsilon1 i in the ISNE model is inversely proportional to the cardinality of the neighbors of i:\n\n<!-- formula-not-decoded -->\n\nProof A formal proof of this theorem is provided in Appendix B.1.3 /intersectionsq /unionsq\n\nA consequence of Theorem 1 is that nodes with a greater number of neighbors tend to have lower noise levels in their representations. This is because the averaging effect inherent in processing information from a larger neighborhood helps to reduce the impact of individual noise components. Consequently, nodes with robust representations possess higher reliability and perform better in downstream tasks that utilize these representations.\n\n## Bias and variance of representation similarity\n\nMany downstream tasks in network analysis rely on the similarity between node representations, rather than the node representations themselves. In this section, the bias and variance of the similarity functions s \u02dc f and s \u02dc h are investigated, which measure the dot-product similarity between representations obtained with noise-corrupted encoders.\n\n## Bias\n\nTheorem 2 The embedding similarity function s \u02dc f ( , i j ) obtained from the noise-corrupted lookup table encoder is unbiased if i = j .\n\n/negationslash\n\nProof A formal proof of this theorem is provided in Appendix B.2 /intersectionsq /unionsq\n\nTheorem 3 The embedding similarity function s \u02dc h ( , i j ) obtained from the noise-corrupted ISNE encoder exhibits bias proportional to the number of common neighbors between nodes i and j:\n\n<!-- formula-not-decoded -->\n\nProof A formal proof of this theorem is provided in Appendix B.4 /intersectionsq /unionsq\n\nHere, | N i , j | denotes the number of common neighbors between nodes i and j, and Tr (/Sigma1 ) represents the trace of the covariance matrix /Sigma1 associated with the noise.\n\nWhile the bias term introduced in Theorem 3 causes the similarity score to deviate from the exact similarity measure of ISNE, it does not necessarily invalidate its utility as an indicator of node proximity. Nodes with a higher number of shared neighbors tend to exhibit a stronger similarity under this bias. The empirical results in Sect. 'Evaluation of transductive task performance' demonstrate that this bias does not adversely affect the performance of downstream tasks.\n\n## Variance\n\nNext, the variance of the representation similarity functions is analyzed, which measures the spread of the similarity scores around their expected values.\n\nTheorem 4 (Variance Bound) The variance of the representation similarity function s \u02dc h is upper-bounded by a constant factor multiplied by the variance of the similarity obtained using the noise-corrupted lookup table encoder, s \u02dc f , for the same nodes:\n\n<!-- formula-not-decoded -->\n\nwhere, K = min {| N i | , | N j |} represents the minimum number of neighbors between nodes i and j.\n\nProof A formal proof of this theorem is provided in Appendix B.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 0,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_0",
      "chunk_index": 0,
      "start_offset": 0,
      "end_offset": 20480,
      "line_start": 0,
      "line_end": 0,
      "token_count": 3086,
      "content_hash": "f19e2c9a3e3b008eea9ecfb5c5e51038",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p1",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": "6 /intersectionsq /unionsq\n\nTheorem 4 establishes a valuable relationship between the variances of the similarity functions. It shows that the variance of the similarity obtained using ISNE is guaranteed to be less than or equal to a constant factor multiplied by the variance of the similarity obtained using the lookup table encoder, for any two nodes in the network. This upper bound serves as a tool for assessing the robustness of downstream tasks that rely on representation similarity, such as Information Retrieval. In the subsequent section, the robustness of such tasks in the presence of noise is explored.\n\n## Robustness of information retrieval\n\n## Impact of noise on retrieval\n\nWhen seeking similar entities in a network, the similarity between their representations often serves as a crucial metric for retrieving relevant items. Understanding how noise affects the retrieval process sheds light on the robustness of the order of node similarities in the presence of parameter noise.\n\nLet q denote a query node, and r and n represent candidate nodes that are relevant and non-relevant to the query, respectively. Information Retrieval (IR) aims to retrieve the relevant item, meaning we want s ( q , r ) &gt; s ( q , n ) , where s denotes the similarity function. The effect of noise can be modelled on the similarity scores by using zero-mean Gaussian noise vectors \u03b4 r \u223c N ( 0 , \u03c3 2 r ) and \u03b4 n \u223c N ( 0 , \u03c3 2 n ) for the relevant and non-relevant nodes, respectively. The robustness of the retrieval process can be assessed by calculating the probability of retrieving the wrong node, given by Eq. 7.\n\n<!-- formula-not-decoded -->\n\nFig. 1 Misclassification probability comparison between lookup table (red) and ISNE models (blue). The x -axis represents the normalized true similarity difference between relevant and non-relevant nodes. The y -axis indicates the probability of incorrect retrieval\n\n<!-- image -->\n\nHere, /Phi1 epresents the cumulative distribution function (CDF) of the standard normal distribution. The denominator follows from the fact \u03b4 n -\u03b4 r = \u03b4 \u223c N ( 0 , \u03c3 2 r + \u03c3 2 n ) .\n\n## Advantage of ISNE in retrieval\n\nUsing Theorem 4, the probability of retrieving the wrong node in the ISNE model can be rewritten as:\n\n<!-- formula-not-decoded -->\n\nFig.1 compares the misclassification probabilities for the lookup table and ISNE models. The x -axis denotes the normalized true similarity difference between relevant and non-relevant nodes, while the y -axis illustrates the probability of incorrect retrieval. As the minimum node degree ( K ) increases, the ISNE model demonstrates a significant improvement in retrieval robustness compared to the lookup table model. This is evident in the steeper decrease in misclassification probability for the ISNE model with increasing K .\n\nThe ISNE model's advantage stems from its lower noise variance, which scales inversely with K compared to the constant noise variance in the lookup table model. In simpler terms, the noise in ISNE has a lesser impact on the final similarity score as the number of neighbors ( K ) increases.\n\nIt is important to note that the derivation intentionally disregarded the bias term introduced in Theorem 3. This omission is justified because the underlying assumptionthat relevant items tend to have a higher degree of overlap compared to non-relevant ones-is generally valid. If this assumption holds, the true difference between s ( q , r ) and s ( q , n ) would be even greater, further bolstering the ISNE model's robustness. Consequently, neglecting the bias term doesn't invalidate our conclusion that the ISNE model exhibits superior robustness in noisy environments.\n\n<!-- image -->\n\nTable 1 Properties of the datasets used in the experiments\n\n| Name             | Nodes   | Edges     |   Classes |   Features |\n|------------------|---------|-----------|-----------|------------|\n| Arxiv [30]       | 169,343 | 1,166,243 |        40 |        128 |\n| Cora [31]        | 19,793  | 126,842   |        70 |       8710 |\n| PubMed [31]      | 19,717  | 88,648    |         3 |        500 |\n| BlogCatalog [32] | 5196    | 343,486   |         6 |       8189 |\n| WikiCS [33]      | 11,701  | 431,726   |        10 |        300 |\n\n## Empirical results\n\nThis section delves into the performance of Inductive ShallowNodeEmbedding(ISNE)throughaseriesofexperiments designed to address the following key research questions:\n\n- 1. Performance in Transductive Tasks: Do ISNE embeddings maintain performance comparable to the traditional lookup table encoder in transductive tasks, where all test nodes are seen during training?\n- 2. Inductive Reasoning: Can ISNE maintain comparable classification accuracy on unseen nodes to its performance on nodes seen during training?\n- 3. Comparison with Inductive Algorithms: How does ISNE compare to other state-of-the-art inductive algorithms designed specifically for handling unseen data?\n\nBy addressing these research questions through carefully designed experiments, we aim to gain a comprehensive understanding of the effectiveness of composite embeddings across various task settings.\n\n## Datasets\n\nTo comprehensively evaluate the effectiveness and generalizability of Inductive Shallow Node Embedding (ISNE), a diverse range of datasets were employed. These datasets, summarized in Table 1, encompass various network types and exhibit distinct structural characteristics.\n\n- \u00b7 Arxiv [30]: The dataset represents the citation network of papers on arXiv.org, covering different subject areas. Nodes represent papers, edges represent citations, and each node is associated with a feature vector derived from the paper's text. The task is to classify papers into 40 different subject areas.\n- \u00b7 Cora [31]: The dataset is a well-known benchmark for citation network analysis. It comprises scientific publications classified into 70 different research areas. Nodes represent papers, and edges denote citation relationships between them. Each node has a feature vector based on\n\nTable 2 Average Accuracy Scores (%) in unattributed Transductive Node Classification\n\nBlog\n\n|             |   Arxiv |   Catalog |   Cora |   PubMed | WikiCS   |\n|-------------|---------|-----------|--------|----------|----------|\n| Node2Vec    |   0.475 |     0.518 |  0.489 |    0.706 | 0.574    |\n| LINE        |   0.614 |     0.633 |  0.528 |    0.759 | 0.758    |\n| ISNE (ours) |   0.557 |     0.657 |  0.569 |    0.774 | 0 . 762  |\n\nBold values present the best performers and underlined values highlight the statistically significant best performer with a confidence level of 95%\n\nthe paper's abstract, consisting of a bag-of-words representation.\n\n- \u00b7 PubMed [31]: The dataset contains a citation network of scientific publications in the biomedical domain. Nodes represent papers, and edges indicate citation links. The task involves classifying papers into three classes related to different diseases. Node features are derived from the Term Frequency-Inverse Document Frequency (TF-IDF) of words in the paper abstracts.\n- \u00b7 BlogCatalog [32]: The dataset is a social network where nodes represent users, and edges represent the friendship relationships between them. The classification task is to assign users to one of six predefined categories. Each user has a descriptor vector as node feature.\n- \u00b7 WikiCS [33]: The dataset consists of a citation network of computer science articles from Wikipedia. Nodes represent articles, and edges denote hyperlinks between them. The classification task involves categorizing articles into 10 different computer science topics. Each node has a feature vector representing the article's content, captured through a pre-trained language model.\n\n## Evaluation of transductive task performance\n\nThis section addresses research question 1, which investigates the performance of the proposed Inductive Shallow Node Embedding (ISNE) method compared to traditional lookup table encoders in transductive tasks. Transductive tasks involve training and testing on the same set of nodes, aiming to evaluate the models' ability to capture inherent network structure and perform well on tasks like node classification.\n\nThe detailed experimental setup and model configurations are provided in Appendix A.1 for reference.\n\nTable 2 shows that ISNE significantly outperforms both LINEandNode2Veconthe BlogCatalog Cora PubMed , , and WikiCS datasets. This superior performance demonstrates ISNE's ability to effectively capture network structure and produce robust embeddings for node classification tasks in these environments.\n\nHowever, on the Arxiv dataset, ISNE falls short comparedtoLINE(whilestillperformingsignificantlybetterthan Node2Vec). This dataset is characterized by extreme sparsity, being an order of magnitude sparser than the other datasets considered. Additionally, the Arxiv dataset exhibits very low node homophily (42%). These factors degrade ISNE's performance, as the method relies on the information from neighboring nodes. High sparsity results in a low number of neighbors, and low homophily means that these neighbors are often less informative about the node's class, impacting the overall effectiveness of ISNE in such conditions.\n\nIn this experiment, ISNE parameters were set to capture a more global node similarity through long random walks, while LINE preserves only 1st and 2nd proximities. It is possible that if the ISNE parameters were adjusted to focus more on local structures (such as LINE), it could achieve a similar level of performance to LINE in datasets with high sparsity and low homophily like Arxiv .\n\n## Evaluation of inductive task performance\n\nThis section tackles research questions 2 and 3, focusing on the performance of ISNE in inductive tasks involving unforeseen nodes. Inductive tasks require models to generalize their knowledge and perform well on unseen data, making them particularly challenging. To comprehensively assess ISNE's effectiveness and generalizability, evaluations on both attributed and unattributed node classification tasks were conducted. The specific details of the experimental setup and model configurations can be found in Appendix A.2 for reference.\n\n## Unattributed node classification\n\nThis section addresses the ability of ISNE to handle unseen nodes in unattributed node classification tasks. Evaluating performance on unseen nodes is crucial to assess the generalizability and extrapolation capabilities of the model.\n\nTable 3 compares the average accuracy scores achieved by ISNE on both training nodes and unseen nodes across different datasets. The last row presents the relative performance of unseen nodes compared to training nodes.\n\nAs shown in Table 3, ISNE demonstrates remarkable generalization capabilities for unseen nodes. The model consistently achieves accuracy exceeding 90% across all datasets, showcasing its ability to effectively construct representations for unseen data points. Notably, the minimal performance drop ( &lt; 2%) in the BlogCatalog PubMed , , WikiCS datasets further emphasizes the robustness and generalizability of ISNE.\n\nThese findings highlight the effectiveness of ISNE in handling unseen nodes, making it a valuable tool for tasks requiring models to perform well on new and evolving data.\n\nTable 3 Average accuracy scores (%) in unattributed inductive node classification\n\n| Blog           | Arxiv   | Catalog   | Cora   | PubMed   | WikiCS   |\n|----------------|---------|-----------|--------|----------|----------|\n| Training nodes | 0.557   | 0.657     | 0.569  | 0.774    | 0.762    |\n| Unseen nodes   | 0.508   | 0.648     | 0.529  | 0.764    | 0.751    |\n| Relative       | 91.2%   | 98.6%     | 92.9%  | 98.7%    | 98.6%    |\n\nTable 4 Average accuracy scores (%) in Attributed Inductive Node Classification\n\ntural patterns within the network and the rich information encapsulated within the node attributes.\n\nBlog\n\n|             | Arxiv   |   Catalog |   Cora |   PubMed |   WikiCS |\n|-------------|---------|-----------|--------|----------|----------|\n| Baseline    | 0.557   |     0.834 |  0.545 |    0.844 |    0.775 |\n| GraphSAGE   | 0.583   |     0.783 |  0.553 |    0.83  |    0.808 |\n| ISNE (ours) | 0 . 585 |     0.873 |  0.605 |    0.865 |    0.816 |\n\nBold values present the best performers and underlined values highlight the statistically significant best performer with a confidence level of 95%\n\n## Attributed node classification\n\nThis section delves into the performance of ISNE on attributed node classification tasks. In this setting, both the model and the benchmark method, GraphSAGE, utilize the sameinformation:ISNEembeddingsconcatenatedwithnode attributes and GraphSAGE embeddings, respectively. This ensures a fair comparison by eliminating bias introduced by different attribute usage. Additionally, a baseline utilizing node attributes only is included to assess the inherent predictive power of attributes, independent of embedding techniques.\n\nTable 4 summarizes the average accuracy scores achieved by the baseline, GraphSAGE, and ISNE on attributed inductive node classification tasks across different datasets.\n\nAs shown in Table 4, ISNE consistently outperforms the baseline and GraphSAGE across all datasets, with statistically significant improvements observed in BlogCatalog , Cora , and PubMed . These findings highlight the effectiveness of combining ISNE embeddings and node attributes for classification tasks.\n\nWhile GraphSAGE integrates node attributes into its embedding generation process, its performance does not always surpass the baseline utilizing attributes alone. This suggests that the unsupervised learning approach used by GraphSAGE may not consistently extract optimal information for node classification as it needs to strike a balance between preserving the original node attribute information and capturing structural relationships within the network.\n\nIn contrast, ISNE effectively captures structural information through its encoder function, and the undistorted node attributes can be seamlessly incorporated for downstream tasks. This allows ISNE to leverage both the inherent struc-\n\n<!-- image -->\n\nIn conclusion, the experimental results demonstrate the competitive advantage of ISNE over both GraphSAGE and attribute-based methods. The significant performance improvements achieved by ISNE showcase its potential as a powerful tool for attributed node classification tasks, offering a valuable alternative to existing approaches.\n\n## Limitations\n\nWhile the proposed method demonstrates significant advancements in handling unseen nodes and adapting to dynamic network structures, several limitations must be acknowledged. It is important to note that these limitations are not unique to ISNE but are inherent to any method for unattributed node representation learning. When additional information is unavailable, there are inherent constraints on performance.\n\n## Dependence on neighboring nodes\n\nISNE embeds new nodes based on the parameter vectors of their neighbors which are learned ahead of time. If a large number of new nodes are introduced to the network and they primarily form edges among themselves rather than with previously existing nodes, the new nodes' embeddings may lack richness and informativeness. This can lead to degraded performance in scenarios where new nodes are densely interconnected but sparsely connected to the existing network, as the embeddings of new nodes may not capture the broader network structure effectively. However, in many realworld graphs, the phenomenon of preferential attachment is observed, where new nodes tend to connect to high-degree nodes [34]. This natural tendency helps mitigate the issue, as connections to well-established, high-degree nodes can enrich the embeddings of new nodes, ensuring they reflect the broader network structure.\n\n## Limited initial information\n\nISNE relies on the neighborhood information of new nodes for embedding. When new nodes have limited initial connec-\n\ntions, especially in the early stages of their introduction, the embeddings generated may be suboptimal. This can result in reduced accuracy and effectiveness of the embeddings in capturing the true position and role of new nodes within the network, particularly when node attributes are sparse or unavailable.\n\nThese limitations are inherent to unattributed node representation learning methods. Without additional information such as node attributes or external context, it is challenging to achieve better performance.\n\n## Conclusion\n\nThis paper introduced Inductive Shallow Node Embedding (ISNE), a novel approach that addresses the limitations of existing shallow embedding methods for learning node representations in graphs. Unlike traditional methods that rely on lookup tables, ISNE utilizes an encoder specifically designed to capture the local neighborhood structure of each node. This approach enables ISNE to effectively generalize to unseen nodes, making it particularly valuable for dynamic network settings.\n\nComprehensive evaluation across various tasks and datasets showcases the effectiveness of ISNE:\n\n- 1. Competitive performance in transductive tasks : ISNE achieves comparable or better performance compared to traditional methods like Node2Vec and LINE in transductive node classification tasks, demonstrating its ability to capture inherent network structure.\n- 2. Superior performance in handling unseen nodes : ISNE exhibits remarkable generalization capabilities, maintaining high accuracy on unseen nodes in inductive tasks. This highlights its ability to construct high-quality representations for new data points.\n- 3. Effective utilization of node attributes : When combined with node attributes, ISNE consistently outperforms the state-of-the-art method, GraphSAGE, in attributed node classification tasks. This demonstrates the effectiveness of ISNE in leveraging both structural information and node attributes for improved performance.\n\nBeyond empirical findings, we also presented a theoretical analysis of the robustness of ISNE to parameter noise:\n\n- 1. Covariance of Additive Noise : One of our contributions is the finding that the covariance of the additive noise term in the ISNE model is inversely proportional to the cardinality of a node's neighbors. This implies that nodes with more neighbors experience lower noise levels in their representations due to the averaging effect of having a larger neighborhood.\n- 2. Biased Node Similarity : Another theoretical contribution pertains to the bias and variance analysis. The embedding similarity function obtained from the noise-corrupted ISNE encoder exhibits bias proportional to the common neighbors of two nodes. This can often be helpful by increasing embedding similarity within nodes that have high neighborhood overlap.\n- 3. Variance of Node Similarity : Our analysis shows that the variance of the representation similarity function in ISNE is upper-bounded by a factor inversely proportional to the minimum number of neighbors of the nodes involved. This means that ISNE embeddings tend to have lower variance, leading to more stable and reliable similarity measures, especially for nodes with larger neighborhoods.\n\nIn conclusion, ISNE establishes itself as a versatile and robust approach for inductive node representation learning. Its ability to handle unseen nodes, effectively utilize node attributes, and achieve strong theoretical guarantees positions ISNE as a promising tool for various graph mining applications, particularly in dynamic and evolving networks.\n\nOur results have the potential for multiple applications, future research directions include Explainable Artificial Intelligence, recommendation systems, social network analysis, citation networks, graph convolutional networks in computer vision [35], combinatorial optimization [36], and robot swarm control [37].\n\n## Appendix A: Experiment details\n\nThis section details the experimental setup and configuration parameters used for training the embedding models.\n\n## A.1 Transductive setting\n\nTo assess model performance, we employed a 5-fold crossvalidation strategy. After training the model and generating the embeddings, we utilized a K-Nearest Neighbors (KNN) classifier for label prediction on the test nodes. This involved a separate train/test split on the embeddings themselves. The KNN classifier utilized dot product similarity to identify the 15 nearest neighbors within the training set embeddings.\n\n## A.2 Inductive setting\n\nIn the inductive setting, the model is evaluated on unseen nodes. We split the nodes into training and test sets, where the test set comprises nodes not present during training. Similar to the transductive setting, we employed a 5-fold cross-validation approach with a KNN classifier.\n\nForattributed inductive node classification tasks, the ISNE embeddings are augmented with the inherent node features.\n\nTable 5 One-sided related T-test P-values for ISNE's higher accuracy scores across various datasets\n\n| Dataset     | Other method   | P value    |\n|-------------|----------------|------------|\n| BlogCatalog | N2V            | 0.000976%  |\n| BlogCatalog | LINE           | 0.878058%  |\n| Arxiv       | N2V            | 0.000592%  |\n| Arxiv       | LINE           | 99.999557% |\n| Cora        | N2V            | 0.000424%  |\n| Cora        | LINE           | 0.014846%  |\n| PubMed      | N2V            | 2.876472%  |\n| PubMed      | LINE           | 0.149597%  |\n| WikiCS      | N2V            | 0.000278%  |\n| WikiCS      | LINE           | 7.716635%  |\n\nTable 6 One-Sided Related T test P values for ISNE's higher accuracy scores across various datasets\n\n| Dataset     | Model 1   | P value    |\n|-------------|-----------|------------|\n| BlogCatalog | GraphSAGE | 0.001334%  |\n| BlogCatalog | Baseline  | 0.026882%  |\n| Arxiv       | GraphSAGE | 22.032741% |\n| Arxiv       | Baseline  | 0.006854%  |\n| Cora        | GraphSAGE | 0.033234%  |\n| Cora        | Baseline  | 0.000788%  |\n| PubMed      | GraphSAGE | 0.001162%  |\n| PubMed      | Baseline  | 0.008831%  |\n| WikiCS      | GraphSAGE | 3.289872%  |\n| WikiCS      | Baseline  | 0.006987%  |\n\nThis is achieved by concatenating the embeddings and features along their dimension. To balance the influence of these two information sources, we introduce an \u03b1 parameter that determines the relative weight given to each component in the final representation.\n\n## A.3 Model configurations\n\nOur models were trained with the following configurations:\n\n- \u00b7 Embedding Dimensionality The dimensionality of the embedding space varied depending on the dataset. For ArXiv, we used 128 dimensions, while BlogCatalog, Cora, PubMed, and WikiCS all employed a 64dimensionalembeddingspace.Notably,allmodelsinherit their embedding dimensionality from the chosen value specified here.\n- \u00b7 LINE Weopted for a factorization-based implementation of LINE as described in [17].\n- \u00b7 Node2Vec and ISNE These models share several hyperparameters. They utilize a context size of 5, a negative sample ratio of 1, and are trained for 200 epochs.\n- \u00b7 GraphSAGE This model leverages the dimensionality of the node features as the size of its input layer. It possesses two hidden layers, each with a dimensionality of 512, and is trained for 50 epochs.\n\n<!-- image -->\n\n## Appendix B: Derivation of results\n\n## B.1 Properties of /SI\n\n## B.1.1 Expectation of /SI\n\n<!-- formula-not-decoded -->\n\nProof Expressing /epsilon1 i in terms of z and leveraging the linearity of expectation, we can establish the validity of the given statement:\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.1.2 Expectation of the inner product of /SI\n\n<!-- formula-not-decoded -->\n\nProof Initially, we write /epsilon1 i and /epsilon1 j in terms of z as follows:\n\n<!-- formula-not-decoded -->\n\nWe begin by factoring out the constants, consolidating the product of sums into a sum of products. Utilizing the linearity of expectation, we subsequently move the expectation operation inward:\n\n<!-- formula-not-decoded -->\n\nByexpressingthe inner product in summation form and interchanging the order of summation with the expectation, we obtain:\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\nIf n 1 = n 2, the expectation is equal to 0 owing to the independence of zn 1 and zn 2 . Consequently, we can combine the first two summations into a single summation, considering the case where n 1 = n 2. Such an occurrence is only possible when summing over the intersection of N i \u2229 N j . The resulting expression takes the following form:\n\n<!-- formula-not-decoded -->\n\nAs the quantity Tr (/Sigma1 ) remains independent of n , it can be extracted and placed outside the summation. Consequently, the sum simplifies to the cardinality of the intersection: | N i \u2229 N j | . Using all this we can rewrite B7 as follows:\n\n<!-- formula-not-decoded -->\n\nAspecial case of B3 emerges when j = i. In this particular scenario, the expectation of the inner product can be written in the following form:\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.1.3 Expectation of the outer product of /SI\n\n<!-- formula-not-decoded -->\n\nProof By writing E ( /epsilon1 i /epsilon1 /latticetop j ) in terms of z we get:\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\nIf n 1 = n 2 then E ( zn 1 z /latticetop n 2 ) = E ( zn 1 ) E ( z /latticetop n 2 ) = 0 due to independence. If n 1 = n 2 then E ( zn 1 z /latticetop n 1 ) = /Sigma1 by definition. Similarly to the expectation of the inner product in B8, n 1 = n 2 is only possible in the intersection, thus the expectation can be equivalently rewritten as:\n\n<!-- formula-not-decoded -->\n\nAspecific case of B10 emerges when j = i . In this particular scenario, the expectation of the outer product can be written in the following form:\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.1.4 Expectation of the cubic form of /SI\n\n<!-- formula-not-decoded -->\n\nProof We begin by writing /epsilon1 i and /epsilon1 j in terms of z :\n\n<!-- formula-not-decoded -->\n\nIn the terms where the condition n 1 = n 2 = n 3 does not hold, the expected value becomes zero, a consequence of the assumed independence. The sum can be rewritten by only considering terms where n 1 = n 2 = n 3. This circumstance is only possible when all variables are within the intersection of N i and N j . Exploiting this condition, we arrive at the following expression:\n\n<!-- formula-not-decoded -->\n\nBy expressing the dot product in summation form and representing the outcome as a vector, we obtain:\n\n<!-- formula-not-decoded -->\n\nIsserlis's theorem states that the expectation of a product involving an odd number of zero-mean Gaussian random\n\nvariables is always zero. Consequently, this theorem provides a conclusive proof for our statement",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 1,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_1",
      "chunk_index": 1,
      "start_offset": 20482,
      "end_offset": 47272,
      "line_start": 0,
      "line_end": 0,
      "token_count": 4002,
      "content_hash": "cb3f0566fca5a3211d45056ef42b00c5",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p2",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": ". /intersectionsq /unionsq\n\n## B.1.5 Expectation of the quartic form of /SI\n\n<!-- formula-not-decoded -->\n\nProof Following a similar approach as before, we start by expressing /epsilon1 i and /epsilon1 j in terms of z . Additionally, we interchange the order of summations and expectations, while extracting the constants to the front:\n\n<!-- formula-not-decoded -->\n\nIf at least one term is independent in the expectation, the expected value becomes zero. Therefore, we will exclusively focus on cases where there are no terms that are independent of the other three terms. This implies that there will be two pairs that correspond to the same nodes-so there is no independent z . We denote these pairs as \u00b5 and \u03c1 . Taking this into consideration B20 can be rewritten as:\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\nThe expectation can be divided into two cases: one where \u00b5 = \u03c1 and another where \u00b5 = \u03c1 . In the subsequent analysis, wewill compute the expectation for each case separately and then reconstruct the overall sum by counting the occurrences of each case.\n\n/negationslash\n\nCase 1: \u00b5 = \u03c1 Leveraging the independence of z \u00b5 and z \u03c1 (given that \u00b5 = \u03c1 ), equation B22 can be expressed as:\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\nCase 2: \u00b5 = \u03c1 Using \u00b5 = \u03c1 we can rewrite B22 as follows:\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\nNow that we have the expectation for \u00b5 = \u03c1 and \u00b5 = \u03c1 , we just need to count how many times each appears in the sum in B20.\n\n/negationslash\n\n- \u00b7 Case 1: there are | N i || N j | + 2 | N i \u2229 N j | 2 - | 3 N i \u2229 N j | terms at total such that \u00b5 = \u03c1 , composed of the following two disjoint components:",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 2,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_2",
      "chunk_index": 2,
      "start_offset": 47274,
      "end_offset": 48954,
      "line_start": 0,
      "line_end": 0,
      "token_count": 309,
      "content_hash": "aa900757fdd2badf6c9721df1a9f0bc8",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p3",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": "\n- - There are | N i || N j | - | N i \u2229 N j | 2 possible combinations for \u00b5 and \u03c1 such that \u00b5 / \u2208 N i \u2229 N j and \u03c1 / \u2208 N i \u2229 N j . These combinations can be constructed from i 1 , i 2 , j 1 , j 2 only if i 1 , i 2 \u2208 N N i \\ i \u2229 N j and j 1 , j 2 \u2208 N j \\ N i \u2229 N j and this further implies that i 1 = i 2 and j 1 = j 2, resulting in 1 term per \u00b5,\u03c1 pair.\n\n/negationslash\n\n- -When \u00b5,\u03c1 \u2208 N i \u2229 N j , there are exactly | N i \u2229 N j | 2 -| N i \u2229 N j | terms when \u00b5 = \u03c1 . However this time there are 3 distinct ways that construct \u00b5,\u03c1 from i 1 , i 2 , j 1 , j 2:\n\n<!-- formula-not-decoded -->\n\n- \u00b7 Case 2: if \u00b5 = \u03c1 , it follows that both \u00b5 and \u03c1 must belong to the intersection of the neighborhoods N i \u2229 N j , implying the existence of a total of | N i \u2229 N j | such terms. Since \u00b5 = \u03c1 can only be true if i 1 = i 2 = j 1 = j 2, the total number of the terms corresponding to Case 2 is | N i \u2229 N j | .\n\n## Putting the results altogether we get\n\n<!-- formula-not-decoded -->\n\nand thus we arrived to the desired form of the quartic expectation.\n\n/intersectionsq /unionsq\n\n## B.2 Expectation of the lookup table embedding similarity ( s \u02dc f )\n\nIf i = j then\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\nProof\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nProof\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\nUsing i = j we get\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.3 Variance of the lookup table embedding similarity ( s \u02dc f )\n\nIf i = j then\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\n/negationslash\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n= \u03b8 /latticetop i /Sigma1 \u03b8 i + \u03b8 /latticetop j /Sigma1 \u03b8 j + 1 /latticetop /Sigma1 2 1 (B40)\n\n<!-- formula-not-decoded -->\n\n## B.4 Expectation of the ISNE embedding similarity ( s \u02dc h )\n\n<!-- formula-not-decoded -->\n\n## Proof\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nE ( s \u02dc h ( , i j ) ) = E ( h i ( ) /latticetop h ( j ) + h i ( ) /latticetop /epsilon1 j + h ( j ) /latticetop /epsilon1 i + /epsilon1 /latticetop i /epsilon1 j ) (B44)\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.5 Variance of the ISNE embedding similarity ( s \u02dc h )\n\n<!-- formula-not-decoded -->\n\nwhere Q = | | N i , j | N i || N j | .\n\n## Proof\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\n## B.6 Variance bound\n\n<!-- formula-not-decoded -->\n\n## Proof\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nThe h i ( ) /latticetop /Sigma1 h ( j ) term can always be upper-bounded by max { h i ( ) /latticetop /Sigma1 h i ( ), h ( j ) /latticetop /Sigma1 h ( j ) } . We will assume, without loss of generality, that h i ( ) /latticetop /Sigma1 h i ( ) \u2265 h ( j ) /latticetop /Sigma1 h ( j ) . The derivation proceeds by bounding h i ( ) /latticetop /Sigma1 h ( j ) with h i ( ) /latticetop /Sigma1 h i ( ) . However, the proof follows an analogous structure in the alternative case where h ( j ) /latticetop /Sigma1 h ( j ) is larger.\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\n<!-- formula-not-decoded -->\n\nIf we posit that both the ISNE and lookup table models are capable of achieving the same optimal solution (w.r.t. the Node2Vec loss function), then we can deduce that the (not noise augmented) embedding vector associated with any node v is identical in both models, denoted by f (v) = h (v) (and is equal the optimal embedding). Using this we can get to the desired upper bound:\n\n<!-- formula-not-decoded -->\n\n/intersectionsq\n\n/unionsq\n\nAuthor Contributions All authors contributed equally to this work.\n\nFunding Open access funding provided by Budapest University of Technology and Economics. The authors declare that no funds, grants, or other support were received during the preparation of this article.\n\nData Availibility All data generated or analyzed during this study are included in this article.\n\n## Declarations\n\nConflict of interest The authors have no relevant financial or nonfinancial interests to disclose.\n\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or other third party material in this article are included in the article's Creative Commons licence, unless indicated otherwise in a credit line to the material. If material is not included in the article's Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit http://creativecomm ons.org/licenses/by/4.0/.\n\n## References\n\n- 1. Hamilton WL, Ying R, Leskovec J (2017) Representation learning on graphs: methods and applications. arXiv preprint arXiv:1709.05584\n- 2. Yi H-C, You Z-H, Huang D-S, Kwoh CK (2022) Graph representation learning in bioinformatics: trends, methods and applications. Briefings Bioinform 23(1):340\n- 3. KimM,BaekSH,SongM(2018)Relationextractionforbiological pathway construction using node2vec. BMC Bioinform 19:75-84\n- 4. Thafar MA, Olayan RS, Albaradei S, Bajic VB, Gojobori T, EssackM,GaoX(2021)Dti2vec:drug-targetinteractionprediction using network embedding and ensemble learning. J Cheminform 13(1):1-18\n- 5. Wang Y, Li Z, Farimani AB (2023) In: Qu, C., Liu, H. (eds.) Graph neural networks for molecules, pp. 21-66. Springer, Cham\n- 6. Wang M, Lin Y, Lin G, Yang K, Wu X-m (2020) M2grl: A multi-task multi-view graph representation learning framework for web-scale recommender systems. In: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 2349-2358\n- 7. Ge S, Wu C, Wu F, Qi T, Huang Y (2020) Graph enhanced representation learning for news recommendation. In: Proceedings of The Web Conference 2020, pp. 2863-2869\n- 8. Liu Y, Tian Z, Sun J, Jiang Y, Zhang X (2020) Distributed representation learning via node2vec for implicit feedback recommendation. Neural Comput Appl 32:4335-4345\n- 9.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 3,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_3",
      "chunk_index": 3,
      "start_offset": 48956,
      "end_offset": 55839,
      "line_start": 0,
      "line_end": 0,
      "token_count": 1189,
      "content_hash": "60c82ac681c520e4806741be45e02626",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p4",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Tan Q, Liu N, Hu X (2019) Deep representation learning for social network analysis. Front Big Data 2:2\n- 10. Li B, Pi D (2020) Network representation learning: a systematic literature review. Neural Comput Appl 32(21):16647-16679\n- 11.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 4,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_4",
      "chunk_index": 4,
      "start_offset": 55841,
      "end_offset": 56077,
      "line_start": 0,
      "line_end": 0,
      "token_count": 38,
      "content_hash": "829867df22820312cd2bad52c53a8dba",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p5",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Grover A, Leskovec J (2016) node2vec: scalable feature learning for networks. In: Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 855-864\n- 12. Ljubi\u02c7 ci\u00b7 K, Mer\u00b7ep A, Kostanj\u02c7 car Z (2023) Churn prediction methc c ods based on mutual customer interdependence. J Comput Sci 67:101940\n- 13. Thang DC, Dat HT, Tam NT, Jo J, Hung NQV, Aberer K (2022) Nature vs. nurture: feature vs. structure for graph neural networks. Pattern Recogn Lett 159:46-53\n- 14. Hamilton W, Ying Z, Leskovec J (2017) Inductive representation learning on large graphs. Adv Neural Inform Process Syst 30\n- 15.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 5,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_5",
      "chunk_index": 5,
      "start_offset": 56079,
      "end_offset": 56717,
      "line_start": 0,
      "line_end": 0,
      "token_count": 105,
      "content_hash": "ebbccd4d8495523084a65229c8a7223d",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p6",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Perozzi B, Al-Rfou R, Skiena S (2014) Deepwalk: Online learning of social representations. In: Proceedings of the 20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 701-710\n- 16. Tang J, Qu M, Wang M, Zhang M, Yan J, Mei Q (2015) Line: largescale information network embedding. In: Proceedings of the 24th International Conference on World Wide Web, pp. 1067-1077\n- 17.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 6,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_6",
      "chunk_index": 6,
      "start_offset": 56719,
      "end_offset": 57123,
      "line_start": 0,
      "line_end": 0,
      "token_count": 65,
      "content_hash": "66e4b537a07eb3de19fa7f5c51147048",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p7",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Qiu J, Dong Y, Ma H, Li J, Wang K, Tang J (2018) Network embedding as matrix factorization: Unifying deepwalk, line, pte, and node2vec. In: Proceedings of the Eleventh ACM International Conference on Web Search and Data Mining, pp. 459-467\n- 18.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 7,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_7",
      "chunk_index": 7,
      "start_offset": 57125,
      "end_offset": 57371,
      "line_start": 0,
      "line_end": 0,
      "token_count": 42,
      "content_hash": "e377d2facb713b4fa7a43b25e9ca5b48",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p8",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Cao S, Lu W, Xu Q (2015) Grarep: learning graph representations with global structural information. In: Proceedings of the 24th ACMInternational on Conference on Information and Knowledge Management, pp. 891-900\n- 19.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 8,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_8",
      "chunk_index": 8,
      "start_offset": 57373,
      "end_offset": 57591,
      "line_start": 0,
      "line_end": 0,
      "token_count": 32,
      "content_hash": "17fc73fdddb9f6cc0c5761c2c606bd66",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p9",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " TangJ, Qu M, Mei Q (2015) Pte: Predictive text embedding through large-scale heterogeneous text networks. In: Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 1165-1174\n- 20.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 9,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_9",
      "chunk_index": 9,
      "start_offset": 57593,
      "end_offset": 57823,
      "line_start": 0,
      "line_end": 0,
      "token_count": 34,
      "content_hash": "dc05f009a2c9d207453aca19b163dc8b",
      "embedding": null
    },
    {
      "id": "pdf_0dbab603_ISNE_paper.pdf_p10",
      "parent": "pdf_0dbab603_ISNE_paper.pdf",
      "parent_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf",
      "type": "academic_pdf",
      "content": " Guo L, Cai X, Qin H, Hao F, Guo S (2022) A content-sensitive citation representation approach for citation recommendation. J Ambient Intell Hum Comput:1-12\n- 21. Zhou H, Sun G, Fu S, Wang L, Hu J, Gao Y (2021) Internet financial fraud detection based on a distributed big data approach with node2vec. IEEE Access 9:43378-43386\n- 22. Ha J, Park S (2022) Ncmd: Node2vec-based neural collaborative filtering for predicting mirna-disease association. IEEE/ACM Trans Comput Biol Bioinform 20(2):1257-1268\n- 23. Ji B-Y, You Z-H, Cheng L, Zhou J-R, Alghazzawi D, Li L-P (2020) Predicting mirna-disease association from heterogeneous information network with grarep embedding model. Sci Rep 10(1):6658\n- 24. Liang X, Si G, Li J, Tian P, An Z, Zhou F (2024) A survey of inductive knowledge graph completion. Neural Comput Appl 36(8):3837-3858\n- 25. Tran DH, Sheng QZ, Zhang WE, Aljubairy A, Zaib M, Hamad SA, Tran NH, Khoa NLD (2021) Hetegraph: graph learning in recommender systems via graph convolutional networks. Neural Comput Appl:1-17\n- 26. Lo WW, Layeghy S, Sarhan M, Gallagher M, Portmann M (2022) E-graphsage: a graph neural network based intrusion detection system for iot. In: NOMS 2022-2022 IEEE/IFIP Network Operations and Management Symposium, pp. 1-9. IEEE\n- 27. Liu J, Lei X, Zhang Y, Pan Y (2023) The prediction of molecular toxicity based on bigru and graphsage. Comput Biol Med 153:106524\n- 28. Sun Q, Wei X, Yang X (2024) Graphsage with deep reinforcementlearning for financial portfolio optimization. Expert Syst Appl 238:122027\n- 29. Liu J, Ong GP, Chen X (2020) Graphsage-based traffic speed forecasting for segment network with sparse data. IEEE Trans Intell Transp Syst 23(3):1755-1766\n- 30. Hu W, Fey M, Zitnik M, Dong Y, Ren H, Liu B, Catasta M, Leskovec J (2020) Open graph benchmark: datasets for machine learning on graphs. Adv Neural Inform Process Syst 33:2211822133\n- 31. Bojchevski A, G\u00fcnnemann S (2017) Deep gaussian embedding of graphs: Unsupervised inductive learning via ranking. arXiv preprint arXiv:1707.03815\n- 32. Tang L, Liu H (2009) Relational learning via latent social dimensions. In: Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 817826\n- 33. Mernyei P, Cangea C (2020) Wiki-cs: A wikipedia-based benchmark for graph neural networks. arXiv preprint arXiv:2007.02901\n- 34. Jeong H, N\u00d8da Z, Barab\u00c6si A-L (2003) Measuring preferential attachment in evolving networks. Europhys Lett 61(4):567\n- 35. Bukumira M, Antonijevic M, Jovanovic D, Zivkovic M, Mladenovic D, Kunjadic G (2022) Carrot grading system using computer vision feature parameters and a cascaded graph convolutional neural network. J Electron Imaging 31(6):061815-061815\n- 36. Schuetz MJ, Brubaker JK, Katzgraber HG (2022) Combinatorial optimization with physics-inspired graph neural networks. Nat Mach Intell 4(4):367-377\n- 37. Tolstaya E, Gama F, Paulos J, Pappas G, Kumar V, Ribeiro A (2020) Learning decentralized controllers for robot swarms with graph neural networks. In: Conference on Robot Learning, pp. 671-682. PMLR\n\n<!-- image -->\n\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.",
      "overlap_context": {
        "pre": "",
        "post": "",
        "position": 10,
        "total": 11
      },
      "symbol_type": "paragraph",
      "name": "paragraph_10",
      "chunk_index": 10,
      "start_offset": 57825,
      "end_offset": 61063,
      "line_start": 0,
      "line_end": 0,
      "token_count": 498,
      "content_hash": "78cca0f5acec45dbde8ff5a702706a13",
      "embedding": null
    }
  ],
  "chunk_count": 11,
  "processing_metadata": {
    "chunking_options": {
      "max_tokens": 1024,
      "output_format": "json",
      "doc_type": "academic_pdf",
      "doc_id": "pdf_0dbab603_ISNE_paper.pdf",
      "path": "/home/todd/ML-Lab/Olympus/HADES-PathRAG/test-data/ISNE_paper.pdf"
    },
    "processing_time": 2.111271858215332,
    "timestamp": "2025-05-18T23:37:16.782887"
  }
}