# HADES-PathRAG: Implementation Tasks

## Priority #1: Standardize Schema Validation and Type Checking with Pydantic v2 âœ…

### Implementation Plan Overview

We need to standardize our approach to schema validation and type checking across the entire codebase by using Pydantic v2 consistently. Currently, we have a mix of TypedDict, Protocol, and Pydantic models across different modules, which creates inconsistency and potential maintenance issues. By centralizing on Pydantic v2, we can enforce both schema validation and static type checking in a single system.

### Migration Strategy

- [x] Create a centralized schema directory
  - [x] Create `src/schemas/` to house all Pydantic models
  - [x] Organize models by domain (documents, embeddings, ISNE, etc.)
  - [x] Ensure proper documentation with examples

- [x] Create a migration roadmap for each module
  - [x] docproc: Move existing Pydantic models to src/schemas
  - [x] embedding: Replace Protocol/TypeVar with Pydantic models
  - [x] chunking: Standardize on Pydantic for all schemas
  - [x] isne: Convert TypedDict definitions to Pydantic models
  - [x] pipelines: Implement Pydantic validation for pipeline inputs/outputs

- [x] Implement schema validation at module boundaries
  - [x] Add validation to module input functions
  - [x] Add validation before data is passed between major components
  - [x] Add validation before persistence to storage
  - [x] Ensure validation errors are properly handled and reported

- [x] Update unit tests
  - [x] Create test fixtures for schema validation
  - [x] Add validation tests for each schema
  - [x] Ensure failure cases are properly tested

- [x] Documentation and tooling
  - [x] Document schema validation approach
  - [x] Add schema export utilities for API documentation
  - [x] Create validation helper utilities for pipeline stages

### Integration Script Improvements âœ…

- [x] Update ArangoDB integration scripts
  - [x] Add explicit database mode control (create vs append)
  - [x] Implement proper force recreation option
  - [x] Fix collection initialization in create mode
  - [x] Update CLI script to work with revised schemas
  - [x] Improve error handling and logging

### Benefits

- Single source of truth for data structures
- Runtime validation and static type checking in one system
- Improved error messages and debugging
- Better IDE support and code completion
- Consistent approach across all modules
- JSON Schema generation for API documentation

### Implementation Approach

1. **Phase 1** âœ…: Create schema directory and migrate existing Pydantic models
2. **Phase 2** âœ…: Convert TypedDict and Protocol interfaces to Pydantic
3. **Phase 3** âœ…: Update integration scripts and CLI tools to work with new schemas
4. **Phase 4** âœ…: Implement validation at module boundaries
5. **Phase 5** âœ…: Update tests and documentation

## Text Storage Module Implementation

### Text Storage Module Implementation Plan Overview

We've implemented and refined the text storage module to better support the HADES-PathRAG architecture, focusing on improving the storage and retrieval of text documents, chunks, and embeddings through ArangoDB. This includes renaming PDF-specific components to handle text documents more generically.

### Core Components

- [x] Create text_storage_readme.md to track documentation for the module
- [x] Rename PDF storage modules to text storage for greater flexibility
  - [x] Rename pdf_storage.py to text_storage.py
  - [x] Rename pdf_repository.py to text_repository.py
  - [x] Update class names and references throughout the codebase
- [x] Implement text document storage with proper typing
  - [x] Add support for storing document metadata
  - [x] Implement chunk storage with embedding support
  - [x] Add relationship handling between documents and chunks
- [x] Support for ISNE embeddings
  - [x] Add embedding_type parameter for different embedding types
  - [x] Implement vector search with embedding type selection
  - [x] Support similarity edge creation based on embeddings
- [x] Implement comprehensive search capabilities
  - [x] Full-text search implementation
  - [x] Vector similarity search
  - [x] Hybrid search (text + vector)

### Quality Assurance Achievements

- [x] Code Review
  - [x] Added unit tests with 85% code coverage for text_storage.py
  - [x] Verified all functions have proper docstrings
  - [x] Added type annotations throughout the module
  - [x] Verified type safety in both service and repository layers
- [x] Documentation Review
  - [x] Created module-level docstrings explaining purpose and usage
  - [x] Added function/class docstrings with parameters, return types
  - [x] Updated text_storage_readme.md with usage examples

### Remaining Tasks

- [ ] Complete unit tests for text_repository.py to reach 85% coverage
- [ ] Add performance benchmarks for vector search operations
- [ ] Create integration tests to verify interaction with the ISNE pipeline
- [ ] Implement better error handling and recovery mechanisms

## Priority #1: ISNE Implementation

### ISNE Implementation Plan Overview

After a review of the initial implementation and the original research paper, we've implemented the ISNE module to accurately reflect the authors' methodology. This implementation follows the comprehensive approach described in the [original paper](https://link.springer.com/article/10.1007/s40747-024-01545-6) and includes a dedicated training pipeline.

### Core ISNE Components

- [x] Create isne_readme.md in the root of the module directory to track documentation
- [x] Set up PyTorch Geometric integration for efficient graph operations
- [x] Implement ISNE layer architecture precisely matching the paper's methodology
- [x] Create specialized loss functions for structure and feature preservation
- [x] Develop training framework with proper neighborhood sampling
- [x] Implement evaluation metrics for model validation
- [x] Build direct pipeline integration rather than file-based loading

### Training Pipeline Implementation

- [x] Created dedicated ISNE training module
  - [x] Implemented TextTrainingPipeline class with modular components
  - [x] Added document processing, chunking, and embedding stages
  - [x] Created graph construction from document chunks
  - [x] Implemented model architecture and training procedures
  - [x] Added checkpoint saving and early stopping
  - [x] Provided comprehensive command-line interface

- [x] Created documentation for training module
  - [x] Added isne_training_readme.md with usage instructions
  - [x] Documented command-line parameters
  - [x] Added code examples for programmatic usage

### Module Structure

```plaintext
src/isne/
  â”œâ”€â”€ __init__.py
  â”œâ”€â”€ isne_readme.md
  â”œâ”€â”€ loaders/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ modernbert_loader.py  # For offline and training processing
  â”‚   â””â”€â”€ graph_dataset_loader.py
  â”œâ”€â”€ layers/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ isne_layer.py
  â”‚   â””â”€â”€ isne_attention.py
  â”œâ”€â”€ models/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â””â”€â”€ isne_model.py
  â”œâ”€â”€ losses/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ structural_loss.py
  â”‚   â”œâ”€â”€ feature_loss.py
  â”‚   â””â”€â”€ contrastive_loss.py
  â”œâ”€â”€ training/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â”œâ”€â”€ sampler.py
  â”‚   â””â”€â”€ trainer.py
  â”œâ”€â”€ evaluation/
  â”‚   â”œâ”€â”€ __init__.py
```

### Pipeline Implementation

We've implemented pipeline integration that supports both direct data flow and batch processing:

- [x] The standard pipeline (docproc â†’ chunking â†’ embedding â†’ ISNE) maintains data in memory
- [x] Added specialized training pipeline for batch processing of document collections
- [x] File writes occur for checkpoints and model persistence
- [x] The `isne_training_text.py` module provides full training workflow
- [x] Optimized for single-threaded CPU resource utilization

## Pipeline Parallelization Implementation

### Parallel Processing Architecture Overview

After reviewing the performance of the current sequential processing pipeline, we need to implement a parallel processing architecture to improve throughput when processing large document collections, particularly for ISNE training which requires processing hundreds of documents.

### Directory Reorganization

- [ ] Restructure orchestration directory for better organization
  - [ ] Create `src/orchestration/` as main orchestration directory
  - [ ] Create `src/orchestration/core/` for shared components
  - [ ] Create `src/orchestration/pipelines/` for specific pipeline implementations
  - [ ] Move relevant files from `src/pipelines/ingest/orchestrator/`

### Pipeline Parallelization Core Components

- [ ] Implement configuration management
  - [ ] Create `src/config/pipeline_config.yaml` for parallel processing settings
  - [ ] Add queue configuration with memory limits and backpressure controls
  - [ ] Implement profile system (high-throughput, balanced, low-memory)
  - [ ] Create config loader with profile selection

- [ ] Develop queue management system
  - [ ] Create `src/orchestration/core/queue_manager.py`
  - [ ] Implement memory-aware queues with backpressure mechanisms
  - [ ] Add monitoring and metrics collection
  - [ ] Implement configurable backoff strategies

- [ ] Implement worker management
  - [ ] Create `src/orchestration/core/parallel_worker.py`
  - [ ] Implement thread pool with controlled concurrency
  - [ ] Add error handling and retry mechanisms
  - [ ] Create monitoring for worker performance

### Pipeline Parallelization Implementation

- [ ] Develop parallel pipeline architecture
  - [ ] Create `src/orchestration/pipelines/parallel_pipeline.py`
  - [ ] Implement document batch processing with thread pools
  - [ ] Add progress tracking and reporting
  - [ ] Create pause/resume capabilities for long-running jobs

- [ ] Create modality-specific pipelines
  - [ ] Implement `src/orchestration/pipelines/text_pipeline.py`
  - [ ] Add ISNE training and enhancement modes
  - [ ] Ensure configuration-driven behavior

### Testing and Documentation

- [ ] Develop comprehensive test suite
  - [ ] Create unit tests for core components (â‰¥85% coverage)
  - [ ] Add integration tests for end-to-end workflows
  - [ ] Implement performance benchmarks for various configurations
  - [ ] Test with varying document collection sizes

- [ ] Create detailed documentation
  - [ ] Add `orchestration_readme.md` with architecture overview
  - [ ] Create usage examples for different scenarios
  - [ ] Document configuration options and their impacts
  - [ ] Add troubleshooting guide for common issues

```plaintext
  â”‚   â”œâ”€â”€ metrics.py
  â”‚   â””â”€â”€ visualizers.py
  â”œâ”€â”€ utils/
  â”‚   â”œâ”€â”€ __init__.py
  â”‚   â””â”€â”€ geometric_utils.py
  â””â”€â”€ pipeline.py
```

### Implementation Phases

#### Phase 1: Core Architecture

- [ ] PyTorch Geometric integration
  - [ ] Set up proper dependencies and imports
  - [ ] Create utilities for PyG data structure handling
- [ ] ISNE Layer Implementation
  - [ ] Implement message passing exactly as in the paper
  - [ ] Create multi-head attention mechanism
  - [ ] Add proper gating mechanisms
- [ ] Base Model Structure
  - [ ] Set up model architecture with skip connections
  - [ ] Implement forward pass with proper layer composition
- [ ] Unit tests for core components
  - [ ] Test layer forward/backward passes
  - [ ] Test message passing operations

#### Phase 2: Training & Evaluation

- [ ] Loss Function Implementation
  - [ ] Implement structural preservation loss
  - [ ] Implement feature preservation loss
  - [ ] Create contrastive loss with negative sampling
- [ ] Training Framework
  - [ ] Create neighborhood sampler for batch training
  - [ ] Implement training loop with learning rate scheduling
  - [ ] Add validation checkpoints
- [ ] Evaluation Metrics
  - [ ] Implement link prediction evaluator
  - [ ] Create node classification metrics
  - [ ] Add embedding visualization utilities
- [ ] Integration tests
  - [ ] Test end-to-end training
  - [ ] Validate with synthetic datasets

#### Phase 3: Pipeline & Integration

- [ ] ModernBERT Integration
  - [ ] Refine and optimize the existing ModernBERT loader
  - [ ] Create pipeline for processing ModernBERT outputs
- [ ] Database Storage
  - [ ] Implement ArangoDB storage for enhanced embeddings
  - [ ] Create retrieval utilities for enhanced embeddings
- [ ] Documentation
  - [ ] Create comprehensive module documentation
  - [ ] Add usage examples and integration guidelines
- [ ] Performance Benchmarks
  - [ ] Benchmark against original paper's results
  - [ ] Measure embedding quality on real documents
  - [ ] Analyze scalability characteristics

### Quality Assurance Checkpoints

After each implementation phase, we will conduct:

1. **Code Review**
   - [ ] Run mypy to check for type errors
   - [ ] Ensure unit test coverage â‰¥ 85%
   - [ ] Verify all functions have docstrings
   - [ ] Remove debugging print statements

2. **Documentation Review**
   - [ ] Verify module-level docstring completeness
   - [ ] Check function/class docstrings with parameters, return types, and examples
   - [ ] Update README and relevant documentation files

3. **Performance Review**
   - [ ] Run benchmarks against reference implementation
   - [ ] Ensure memory usage is reasonable
   - [ ] Verify scalability on larger graphs

### Validation Criteria

- Unit tests with â‰¥85% coverage for all modules
- Integration tests with real document examples
- Performance benchmarks matching the paper's reported metrics
- Comprehensive documentation with usage examples and theoretical explanations

## Previous Pre-Embedding Model Implementation Tasks

This document outlines the critical tasks that must be completed before proceeding with embedding model implementation. Each section contains detailed action items, implementation steps, and validation criteria.

## 1. JSON Object Schema Standardization

### JSON Object Schema Standardization

- [x] Define comprehensive JSON schema using Pydantic
- [x] Implement validation checkpoints throughout the pipeline
- [x] Create schema versioning mechanism

### Implementation Steps for Schema

1. âœ… Create dedicated schema module in `src/schema/document_schema.py`
2. âœ… Define base document schema with required fields
3. âœ… Create specialized schemas for different document types (code, text, etc.)
4. âœ… Add validation utilities for schema verification
5. âœ… Implement automatic schema version upgrades

### JSON Schema Current Status

- Implemented `DocumentSchema`, `DocumentRelationSchema`, `DatasetSchema`, and `ChunkMetadata` Pydantic models
- Updated to use Pydantic v2 configuration exclusively, replacing deprecated features
- Created validation module with functions for validating documents and datasets
- Implemented `ValidationCheckpoint` decorator for pipeline validation
- Added `upgrade_schema_version` function for backward compatibility
- Unit tests for schema and validation modules created

### JSON Schema Completed Tasks

- âœ… Complete and finalize unit tests for schema and validation modules
  - âœ… Created test_document_schema.py to test all schema models
  - âœ… Finalized test_validation.py with tests for all validation functions
  - âœ… Added test_validation_extended.py with additional validation tests
- âœ… Run mypy on schema modules to check for type errors
  - âœ… Ensured all Pydantic models have proper type annotations
  - âœ… Fixed type errors identified by mypy
- âœ… Run test suite with coverage to ensure â‰¥85% coverage
  - âœ… Achieved 93% test coverage for schema and chunking modules
  - âœ… Added tests for edge cases and error handling
- âœ… Integrated chunking module with schema validation
  - âœ… Fixed MockHaystackModelEngine in integration tests
  - âœ… Added proper type annotations to chunking modules
  - âœ… Created integration tests for chunking and validation

### Remaining Tasks

- âœ… Integrate validation checkpoints into the ingestion pipeline
  - âœ… Added validation to document processing pipeline in src/docproc/core.py
  - âœ… Implemented schema validation in the document processing workflow
- [ ] Extend JSON schema for embedding and ISNE modules
  - [ ] Define embedding output schema with vector validation
  - [ ] Create ISNE graph structure schemas
  - [ ] Add relationship type definitions for ISNE
  - [ ] Implement validation for embedding vectors and dimensions
- [ ] Add validation checkpoints to the ISNE pipeline
  - [ ] Validate input embeddings from ModernBERT
  - [ ] Verify graph structure integrity
  - [ ] Validate enhanced embeddings output

## 7. Type Safety Implementation

### Type Safety Primary Tasks

- [ ] Complete type safety implementation across the codebase following the roadmap
- [ ] Fix all mypy errors in key modules
- [ ] Ensure consistent type annotations throughout the codebase

### Implementation Steps for Type Safety

1. âœ… Create centralized typing module with common type definitions
2. âœ… Fix base embeddings interface
3. âœ… Fix embedding adapters
4. âœ… Fix storage interfaces
5. âœ… Fix ArangoDB implementation
6. âœ… Fix graph interfaces
7. âœ… Fix Haystack engine module type safety issues
8. [ ] Fix ISNE pipeline
   - [ ] Add comprehensive type annotations to ISNE models
   - [ ] Implement proper typing for graph structures
   - [ ] Add type validation for vector operations
   - [ ] Create typed interfaces for module interactions
9. [ ] Fix integration points

### Type SafetyCurrent Status

- Completed type safety implementation for the Haystack engine module
- Fixed all mypy errors in Haystack client, server, and engine classes
- Added proper type annotations for all functions in the module
- Improved error handling to maintain consistent state

### Type Safety Completed Tasks

- âœ… Fix type safety issues in src/model_engine/engines/haystack/__init__.py
  - âœ… Added proper type annotations for all methods
  - âœ… Fixed unreachable code errors identified by mypy
  - âœ… Improved class attribute type definitions
  - âœ… Enhanced error handling with proper type checking
- âœ… Fix type safety issues in src/model_engine/engines/haystack/runtime/__init__.py
  - âœ… Added missing imports from typing module
  - âœ… Fixed function parameter type annotations
  - âœ… Added proper handling of Optional parameters
- âœ… Fix type safety issues in src/model_engine/engines/haystack/runtime/server.py
  - âœ… Added type annotations for all functions
  - âœ… Fixed unreachable code in _get_model_info function
  - âœ… Refactored code to use helper functions for better type safety
  - âœ… Enhanced model info structure for consistent typing

### Remaining Type Safety Tasks

- [ ] Fix MCP server tools module typing issues
- [ ] Complete ISNE pipeline type safety implementation
- [ ] Address type safety in integration points between modules
  - âœ… Added error handling for validation failures with proper logging

### Validation Criteria for Schema

- All schema classes have complete type annotations
- Schema validation catches malformed documents
- Unit tests cover various document scenarios
- Schema versioning handles backward compatibility

## 2. Chunking System Validation

### Completed Chunking Tasks

- âœ… Create test suite for chunking validation
  - âœ… Implemented comprehensive tests for AST code chunker
  - âœ… Implemented comprehensive tests for Chonky text chunker
  - âœ… Created integration tests with schema validation
- âœ… Implement different chunking strategies
  - âœ… Implemented code-aware chunking via AST chunker
  - âœ… Implemented semantic chunking via Chonky chunker
- âœ… Enhance Chonky chunker configuration
  - âœ… Added overlap context structure to preserve content surrounding chunks
  - âœ… Implemented device-specific caching to prevent collisions
  - âœ… Added early availability checks for model engine
  - âœ… Created comprehensive documentation in chunker_configuration.md
- âœ… Achieved >85% test coverage for chunking module (currently at 93%)

### Remaining Chunking Tasks

- [x] Validate chunking logic across all supported document types
  - [x] Add tests for PDF documents via Docling
  - [x] Add tests for TXT documents
  - [x] Add tests for CSV, XML, JSON, YAML, and TOML documents
- [ ] Ensure semantic coherence in chunks
  - [ ] Implement metrics for semantic coherence using Chonky
  - [ ] Add tests that verify semantic integrity of chunks
- [x] Implement chunk boundary verification
  - [x] Add validation for context preservation in text chunks
  - [x] Add validation for function/class boundary preservation in code chunks
  - [x] Create tests for boundary edge cases
- [x] Fix type errors and unreachable statements in chunking modules
  - [x] Fix unreachable statements in get_model_engine function
  - [x] Add proper type checking for client attributes
  - [x] Update Pydantic model handling to use model_dump() with fallback
  - [x] Fix client.ping() None reference errors
  - [x] Resolve type incompatibilities in dict returns
  - [x] Ensure all mypy checks pass with no errors
  - [x] Improve error handling with structured helper functions
- [ ] Create visualization tools for chunk analysis
  - [ ] Implement chunk distribution visualizer for token distribution
  - [ ] Create chunk overlap visualization tool for semantic overlap

### Chunking System Recent Updates

- âœ… Enhanced Chonky integration with Haystack model engine
  - âœ… Fixed cache key management to ensure proper model loading
  - âœ… Improved error handling for edge cases with missing models
  - âœ… Added additional logging to track model loading status
- âœ… Optimized resource utilization in chunking system
  - âœ… Implemented caching to prevent redundant model loading
  - âœ… Added proper cleanup of resources when models are no longer needed
- âœ… Enhanced documentation of chunking system architecture
  - âœ… Added detailed explanations of chunking strategy selection

### Validation Criteria for Chunking

- Chunks maintain semantic coherence
- Code chunks preserve function/class boundaries
- Chunk size distribution follows expected patterns
- Chunk overlap is consistent and appropriate
- Test coverage â‰¥ 85% for chunking module

## 3. Metadata Enhancement and Standardization

### Metadata Standardization Primary Tasks

- [ ] Define comprehensive metadata schema
- [ ] Ensure source document linkage in all chunks
- [ ] Add position tracking (offsets) to chunks
- [ ] Include model information in metadata
- [ ] Add ISNE-specific metadata fields
  - [ ] Define graph relationship metadata schema
  - [ ] Track ISNE model version and parameters
  - [ ] Add metrics for embedding enhancement comparison
  - [ ] Include provenance tracking for training data

### Implementation Steps for Metadata

1. Create metadata standardization module in `src/schema/metadata.py`
2. Implement metadata validators and enrichers
3. Add source tracking with unique identifiers
4. Create utilities for metadata extraction and verification

### Validation Criteria for Metadata

- All chunks have complete metadata
- Source document is always traceable
- Position information is accurate
- Metadata format is consistent across document types

## 4. Model Management Infrastructure

### Model Management Primary Tasks

- [ ] Create model loading abstraction layer
- [ ] Implement model caching mechanism
- [ ] Develop model version tracking
- [ ] Add resource monitoring for model usage

### Implementation Steps for Model Management

1. Design model manager interface in `src/model_engine/model_manager.py`
2. Implement Haystack-based model loader
3. Add caching with configurable memory limits
4. Create model registry for version tracking
5. Implement resource monitoring hooks

### Validation Criteria for Model Management

- Models load and unload efficiently
- Caching reduces redundant model loading
- Resource usage stays within defined limits
- Model versioning prevents inconsistencies
- Unit tests verify model management behavior

### Model Management Recent Updates

- âœ… Implemented ModernBERT embedding adapter for CPU inference
  - âœ… Created direct HuggingFace transformers integration without requiring GPU
  - âœ… Added configuration support via embedding_config.yaml
  - âœ… Implemented multiple pooling strategies (cls, mean, max)
  - âœ… Added batch processing to optimize throughput
  - âœ… Enabled proper CPU/GPU detection and configuration
- âœ… Enhanced Haystack model engine integration
  - âœ… Fixed model caching issues for embedding generation
  - âœ… Improved error handling for model loading failures
  - âœ… Added detailed diagnostics for troubleshooting model loading issues

## 5. Pipeline Logging and Monitoring System

### Logging and Monitoring Primary Tasks

- [ ] Implement structured logging framework
- [ ] Add performance monitoring hooks
- [ ] Create pipeline stage tracking
- [ ] Develop error aggregation system
- [ ] Add ISNE-specific logging and monitoring
  - [ ] Implement graph construction progress tracking
  - [ ] Add metrics for embedding enhancement performance
  - [ ] Create visualization for graph relationship structure
  - [ ] Track memory usage during ISNE processing

### Implementation Steps for Logging

1. Configure structured logging in `src/utils/logging.py`
2. Add performance monitoring decorators
3. Implement pipeline stage tracking
4. Create error collection and reporting mechanism
5. Add visualization tools for log analysis

### Validation Criteria for Logging

- Logs capture all critical pipeline operations
- Performance metrics are consistently tracked
- Error reporting provides actionable information
- Log levels are appropriately used

## 6. Performance Benchmarking Framework

### Benchmarking Primary Tasks

- [ ] Develop benchmarking framework
- [ ] Identify and measure pipeline bottlenecks
- [ ] Create performance baseline
- [ ] Implement optimization strategies

### Implementation Steps for Benchmarking

1. Create benchmarking utilities in `src/utils/benchmarking.py`
2. Define key performance indicators (KPIs)
3. Implement measurement points throughout pipeline
4. Develop reporting mechanism for performance results
5. Create optimization plan based on findings

### Validation Criteria for Benchmarking

- Benchmark results are reproducible
- All pipeline stages have performance metrics
- Bottlenecks are clearly identified
- Optimization priorities are established

## 7. ArangoDB Integration Enhancement

### ArangoDB Integration Primary Tasks

- [ ] Define vector field schema for ArangoDB
- [ ] Implement vector indexing strategy
- [ ] Create collection management tools
- [ ] Develop query utilities for vector operations

### Implementation Steps for ArangoDB

1. Extend ArangoDB adapter in `src/db/arango_adapter.py`
2. Define vector field specifications
3. Implement index creation for vector fields
4. Create utilities for vector operations
5. Add test suite for vector operations

### Validation Criteria for ArangoDB

- Vector fields are properly defined
- Indexes support efficient vector operations
- Collection management handles vector data
- Query performance meets requirements
- Test coverage â‰¥ 85% for ArangoDB vector operations

## 8. Documentation and Knowledge Management

### Documentation Primary Tasks

- [ ] Update pipeline documentation
- [ ] Document JSON schema and validation process
- [ ] Create embedding strategy documentation
- [ ] Update developer guidelines

### Implementation Steps for Documentation

1. Update README.md with current architecture
2. Create dedicated documentation for embedding pipeline
3. Document JSON schema with examples
4. Create embedding strategy guide
5. Update developer onboarding documentation

### Validation Criteria for Documentation

- Documentation is comprehensive and accurate
- Examples demonstrate key concepts
- Schema documentation includes all fields
- Developer guidelines facilitate onboarding

## 8. GPU Pipeline Test Coverage

### GPU Pipeline Test Coverage Tasks

- [x] Enhance test coverage for GPU-orchestrated pipeline
  - [x] Improve DocProcStage test coverage (now at 91%)
  - [x] Improve ChunkStage test coverage (now at 93%)
  - [x] Ensure StageBase has 100% test coverage
  - [x] Maintain orchestrator test coverage above 85% (currently at 87%)
- [x] Fix test issues and edge cases
  - [x] Fix tests for document processing with unsupported formats
  - [x] Add tests for timeout handling in document processing
  - [x] Add tests for invalid results from document processing
  - [x] Add tests for error handling in chunking stage
- [x] Address deprecation warnings
  - [x] Update Pydantic V1 style validators to V2 in engine_config.py
  - [x] Replace class Config with ConfigDict
  - [x] Update parse_obj() to model_validate()

### Implementation Steps for GPU Pipeline Testing

1. âœ… Identify modules with insufficient coverage
2. âœ… Create targeted unit tests for uncovered code
3. âœ… Implement tests for error handling and edge cases
4. âœ… Fix deprecation warnings in configuration modules
5. âœ… Verify overall test coverage meets or exceeds 85%

### Validation Criteria for GPU Pipeline Testing

- All critical paths have dedicated tests
- Edge cases are properly tested
- Type safety is enforced throughout the pipeline
- Overall test coverage is at least 85% (currently at 91%)

## 9. Test Coverage Enhancement

### Test Coverage Primary Tasks

- [ ] Achieve â‰¥85% test coverage across all modules
  - [x] Schemas module: Improved coverage from ~70% to 97%
  - [x] Common types module: Improved coverage from 53% to 68%
  - [ ] Remaining modules: Continue improving to reach 85% target
- [ ] Implement integration tests for all critical paths
  - [x] Integration scripts (CLI and orchestrator)
  - [ ] Database transactions and queries
  - [ ] End-to-end pipeline flow
- [ ] Create performance regression tests
- [ ] Add edge case testing for error handling
  - [x] Schema validation edge cases
  - [ ] Network and I/O failure scenarios

### Implementation Steps for Test Coverage

1. âœ… Identify modules with insufficient coverage
2. âœ… Create targeted unit tests for uncovered code
   - Added comprehensive tests for UUIDStr validation
   - Added tests for schema validation with both enum objects and primitive values
   - Created tests for type aliases and embedding vectors
3. ðŸ”„ Implement integration tests for module interactions
   - Updated CLI integration script to properly use new schema components

## 9. Future Enhancements and Optimizations

### Document Processing Pipeline Generalization

- [ ] Create generalized document processing framework across modules
- [ ] Implement consistent data loading and output generation patterns
- [ ] Standardize error handling and validation

### Implementation Steps for Pipeline Generalization

1. Create a common document processor base class in `src/utils/document_processor.py`
2. Implement standardized input/output methods that preserve module-specific behavior
3. Create a unified configuration system that supports module-specific options
4. Refactor existing modules to use the common base while maintaining their specialized functionality
5. Ensure special case handling for `src/docproc` which handles diverse input formats

### Expected Benefits

- Reduced code duplication across modules
- Consistent error handling and validation
- Simplified onboarding for new developers
- Easier testing and maintenance

1. Add performance regression tests

2. Create edge case tests for error handling

### Validation Criteria for Test Coverage

- All critical paths have dedicated tests
- Integration tests validate end-to-end functionality
- CI/CD pipeline catches regressions

## 10. Pre-Launch Verification

### Verification Primary Tasks

- [ ] Perform end-to-end pipeline validation
- [ ] Verify resource utilization
- [ ] Conduct data quality assessment
- [ ] Complete security review

### Implementation Steps for Verification

1. Create validation script for end-to-end testing
2. Implement resource monitoring for full pipeline
3. Develop data quality metrics and validation
4. Conduct security review of pipeline

### Validation Criteria for Verification
  
- Pipeline functions end-to-end without errors
- Resource utilization is within acceptable limits
- Data quality meets defined standards
- Security considerations are addressed

## 11. GPU-Orchestrated Batch Engine

### Overview

Design and implement an asynchronous, double-buffered, multi-GPU engine that moves batches through DocProc, Chunking, Embedding and ISNE stages with configurable stage-to-GPU layout.

### GPU Engine Primary Tasks

- [ ] Create `engine_config.yaml` with pipeline layout, batch size, queue depth, NVLink flag
- [ ] Implement `engine_config.py` (Pydantic / dataclass loader)
- [ ] Define shared `PipelineBatch` dataclass in `src/engine/batch_types.py`
- [ ] Build stage wrappers under `src/engine/stages/` for DocProc, Chunk, Embed, ISNE
- [ ] Develop `StageBase` with device selection & CUDA stream handling
- [ ] Implement `orchestrator.py` (async queues, double-buffer, NVLink peer copy)
- [ ] Add CLI command `ingestor orchestrate` with config path & input path
- [ ] Add Prometheus metrics (`gpu_util`, queue length, batch latency)
- [ ] Write unit tests for config loader, batch types, dummy orchestrator run (â‰¥85 % coverage)
- [ ] Add integration smoke test with 10 docs verifying Arango writes
- [ ] Document architecture in `docs/engine/architecture.md`

### Implementation Steps

1. **Config Layer** â€“ add YAML + loader with validation.
2. **Types** â€“ create `PipelineBatch` and ensure mypy passes.
3. **Stage Wrappers** â€“ thin adapters calling existing modules.
4. **Orchestrator Skeleton** â€“ async queues with dummy `sleep` to prove overlap.
5. **GPU Integration** â€“ add CUDA streams, NVLink copy, real model calls.
6. **Metrics & CLI** â€“ expose metrics, wire into command-line.
7. **Tests & Docs** â€“ ensure coverage, write architecture docs.

### Implementation Validation Criteria

- Pipeline sustains \>90 % utilisation on both RTX A6000 GPUs
- End-to-end latency per 128-doc batch \< 1.5Ã— sum of slowest two stages
- Models remain resident (no load/unload in steady state)
- Unit + integration tests â‰¥ 85 % coverage, mypy clean

## 12. Embedding Configuration and Adapter System

### Embedding System Implementation Status

1. **Embedding Configuration System**
   - [x] Implemented `embedding_config.py` and `embedding_config.yaml` for flexible model configuration
   - [x] Created adapter registration and factory system for easy switching between models
   - [x] Added configuration validation using Pydantic
   - [x] Implemented environment variable support for configuration overrides

2. **Embedding Adapters Implementation**
   - [x] Created ModernBERT adapter for CPU inference
     - [x] Implemented direct HuggingFace transformers integration
     - [x] Added batching support for efficient processing
     - [x] Implemented multiple pooling strategies (cls, mean, max)
   - [x] Maintained CPU adapter for lightweight embedding
   - [x] Fixed type safety issues in embedding interfaces
   - [x] Added extensive logging for troubleshooting

3. **PDF Pipeline Integration**
   - [x] Updated PDF pipeline to use configurable embedding adapters
   - [x] Added validation testing for full pipeline with ModernBERT embedding
   - [x] Created test framework for comparing embedding approaches
   - [x] Implemented CPU-based embedding for resource-constrained environments

### Remaining Embedding Implementation Tasks

1. **Complete Unit Testing of Embedding Modules**
   - [ ] Create dedicated unit tests for ModernBERT adapter
   - [ ] Add tests for embedding configuration system
   - [ ] Ensure â‰¥85% test coverage for all embedding modules

2. **Finalize Model Engine Management**
   - [ ] Complete remaining type fixes in Haystack model engine
   - [ ] Resolve unreachable statements in model engine code
   - [ ] Add proper runtime error handling and recovery
   - [ ] Create standardized model loading interface

3. **Metadata Enhancement**
   - [ ] Implement position tracking for text chunks
   - [ ] Add semantic coherence metrics
   - [ ] Standardize metadata format across document types
   - [ ] Ensure source document linkage in all chunks

4. __Document Processing Pipeline__
   - [x] Test docproc module with all supported document types
   - [x] Verify compatibility with schema standards
   - [x] Integrate with chunking validation
   - [x] Add support for PDF documents via Docling
   - [x] Enhance error handling for document processing failures

### Implementation Timeline

- Week 1-2: Chunking module unit tests and type fixes (completed)
- Week 2-3: Model engine management and metadata enhancements (completed)
- Week 3-4: Document processing pipeline and embedding system implementation (completed)
- Week 5: Performance optimization and benchmarking (in progress)

### CPU-Focused Implementation Strategy

- [x] Prioritized CPU implementation for chunking and embedding systems
  - [x] Created CPU-optimized ModernBERT adapter for inference without GPU requirements
  - [x] Enhanced Chonky semantic chunking with proper CPU fallback
  - [x] Implemented configuration system to easily switch between CPU and GPU modes
  - [x] Focused on optimizing performance on CPU before scaling to GPU
  
### ArangoDB Integration Enhancements

- [x] Enhanced ArangoDB integration for document storage
  - [x] Added explicit collection management in ingestion pipeline
  - [x] Implemented modes for creating new collections vs. appending to existing ones
  - [x] Added proper verification of database structures during ingestion
  - [x] Created comprehensive test suite for ArangoDB adapter
  - [x] Ensured all collections and graphs are properly created

## Next Steps After Completion

Once all these tasks are completed and validated, we can proceed with confidence to:

1. Implement the embedding model integration
2. Scale the system for production usage
3. Optimize the pipeline for specific use cases
4. Enhance the system with advanced features
